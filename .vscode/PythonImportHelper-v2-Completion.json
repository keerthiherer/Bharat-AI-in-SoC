[
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "ast",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ast",
        "description": "ast",
        "detail": "ast",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "contextlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "contextlib",
        "description": "contextlib",
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "IntEnum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "IntEnum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "auto",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "IntEnum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "pathlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pathlib",
        "description": "pathlib",
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "sha256",
        "importPath": "hashlib",
        "description": "hashlib",
        "isExtraImport": true,
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "sha256",
        "importPath": "hashlib",
        "description": "hashlib",
        "isExtraImport": true,
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "ContextManager",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "SupportsIndex",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "chain",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "AutoConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "prod",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "gguf",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gguf",
        "description": "gguf",
        "detail": "gguf",
        "documentation": {}
    },
    {
        "label": "MistralTokenizerType",
        "importPath": "gguf.vocab",
        "description": "gguf.vocab",
        "isExtraImport": true,
        "detail": "gguf.vocab",
        "documentation": {}
    },
    {
        "label": "MistralVocab",
        "importPath": "gguf.vocab",
        "description": "gguf.vocab",
        "isExtraImport": true,
        "detail": "gguf.vocab",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "struct",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "struct",
        "description": "struct",
        "detail": "struct",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "LazyTorchTensor",
        "importPath": "convert_hf_to_gguf",
        "description": "convert_hf_to_gguf",
        "isExtraImport": true,
        "detail": "convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ModelBase",
        "importPath": "convert_hf_to_gguf",
        "description": "convert_hf_to_gguf",
        "isExtraImport": true,
        "detail": "convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "GGUFValueType",
        "importPath": "gguf.constants",
        "description": "gguf.constants",
        "isExtraImport": true,
        "detail": "gguf.constants",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "json,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json.",
        "description": "json.",
        "detail": "json.",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "LogisticRegression",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "listen_loop",
        "importPath": "wake_vosk",
        "description": "wake_vosk",
        "isExtraImport": true,
        "detail": "wake_vosk",
        "documentation": {}
    },
    {
        "label": "listen_loop",
        "importPath": "wake_vosk",
        "description": "wake_vosk",
        "isExtraImport": true,
        "detail": "wake_vosk",
        "documentation": {}
    },
    {
        "label": "listen_for_wake",
        "importPath": "wake_fast",
        "description": "wake_fast",
        "isExtraImport": true,
        "detail": "wake_fast",
        "documentation": {}
    },
    {
        "label": "system_info",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "system_info",
        "description": "system_info",
        "detail": "system_info",
        "documentation": {}
    },
    {
        "label": "speak",
        "importPath": "tts_piper",
        "description": "tts_piper",
        "isExtraImport": true,
        "detail": "tts_piper",
        "documentation": {}
    },
    {
        "label": "Llama",
        "importPath": "llama_cpp",
        "description": "llama_cpp",
        "isExtraImport": true,
        "detail": "llama_cpp",
        "documentation": {}
    },
    {
        "label": "nlu",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nlu",
        "description": "nlu",
        "detail": "nlu",
        "documentation": {}
    },
    {
        "label": "KnowledgeBase",
        "importPath": "knowledge_base",
        "description": "knowledge_base",
        "isExtraImport": true,
        "detail": "knowledge_base",
        "documentation": {}
    },
    {
        "label": "string",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "string",
        "description": "string",
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "psutil,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "psutil.",
        "description": "psutil.",
        "detail": "psutil.",
        "documentation": {}
    },
    {
        "label": "platform",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "platform",
        "description": "platform",
        "detail": "platform",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "wave",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "wave",
        "description": "wave",
        "detail": "wave",
        "documentation": {}
    },
    {
        "label": "PiperVoice",
        "importPath": "piper",
        "description": "piper",
        "isExtraImport": true,
        "detail": "piper",
        "documentation": {}
    },
    {
        "label": "SynthesisConfig",
        "importPath": "piper.voice",
        "description": "piper.voice",
        "isExtraImport": true,
        "detail": "piper.voice",
        "documentation": {}
    },
    {
        "label": "get_intent",
        "importPath": "intent_predict",
        "description": "intent_predict",
        "isExtraImport": true,
        "detail": "intent_predict",
        "documentation": {}
    },
    {
        "label": "pyaudio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pyaudio",
        "description": "pyaudio",
        "detail": "pyaudio",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "vosk",
        "description": "vosk",
        "isExtraImport": true,
        "detail": "vosk",
        "documentation": {}
    },
    {
        "label": "KaldiRecognizer",
        "importPath": "vosk",
        "description": "vosk",
        "isExtraImport": true,
        "detail": "vosk",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "vosk",
        "description": "vosk",
        "isExtraImport": true,
        "detail": "vosk",
        "documentation": {}
    },
    {
        "label": "KaldiRecognizer",
        "importPath": "vosk",
        "description": "vosk",
        "isExtraImport": true,
        "detail": "vosk",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "SentencePieceTokenTypes",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class SentencePieceTokenTypes(IntEnum):\n    NORMAL = 1\n    UNKNOWN = 2\n    CONTROL = 3\n    USER_DEFINED = 4\n    UNUSED = 5\n    BYTE = 6\nclass ModelType(IntEnum):\n    TEXT = 1\n    MMPROJ = 2",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ModelType",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ModelType(IntEnum):\n    TEXT = 1\n    MMPROJ = 2\nAnyModel = TypeVar(\"AnyModel\", bound=\"type[ModelBase]\")\nclass ModelBase:\n    _model_classes: dict[ModelType, dict[str, type[ModelBase]]] = {\n        ModelType.TEXT: {},\n        ModelType.MMPROJ: {},\n    }\n    dir_model: Path",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ModelBase",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ModelBase:\n    _model_classes: dict[ModelType, dict[str, type[ModelBase]]] = {\n        ModelType.TEXT: {},\n        ModelType.MMPROJ: {},\n    }\n    dir_model: Path\n    ftype: gguf.LlamaFileType\n    fname_out: Path\n    is_big_endian: bool\n    endianess: gguf.GGUFEndian",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "TextModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class TextModel(ModelBase):\n    model_type = ModelType.TEXT\n    hf_arch: str\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if not self.is_mistral_format:\n            self.hf_arch = get_model_architecture(self.hparams, self.model_type)\n        else:\n            self.hf_arch = \"\"\n        if \"text_config\" in self.hparams:",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "MmprojModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class MmprojModel(ModelBase):\n    model_type = ModelType.MMPROJ\n    model_arch = gguf.MODEL_ARCH.MMPROJ\n    preprocessor_config: dict[str, Any]\n    global_config: dict[str, Any]\n    n_block_keys = [\"n_layers\", \"num_hidden_layers\", \"n_layer\", \"num_layers\", \"depth\", \"encoder_layers\"]\n    has_vision_encoder: bool = True # by default\n    has_audio_encoder: bool = False\n    # for models having multiple encoders, we need to separate their hparams\n    hparams_vision: dict[str, Any] | None = None",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "GPTNeoXModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class GPTNeoXModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.GPTNEOX\n    def set_gguf_parameters(self):\n        self.gguf_writer.add_context_length(self.hparams[\"max_position_embeddings\"])\n        self.gguf_writer.add_embedding_length(self.hparams[\"hidden_size\"])\n        self.gguf_writer.add_block_count(self.block_count)\n        self.gguf_writer.add_feed_forward_length(self.hparams[\"intermediate_size\"])\n        self.gguf_writer.add_rope_dimension_count(\n            int(self.hparams[\"rotary_pct\"] * (self.hparams[\"hidden_size\"] // self.hparams[\"num_attention_heads\"])),\n        )",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "BloomModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class BloomModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.BLOOM\n    def set_gguf_parameters(self):\n        n_embed = self.hparams.get(\"hidden_size\", self.hparams.get(\"n_embed\"))\n        n_head = self.hparams.get(\"n_head\", self.hparams.get(\"num_attention_heads\"))\n        self.gguf_writer.add_context_length(self.hparams.get(\"seq_length\", n_embed))\n        self.gguf_writer.add_embedding_length(n_embed)\n        self.gguf_writer.add_feed_forward_length(4 * n_embed)\n        self.gguf_writer.add_block_count(self.block_count)\n        self.gguf_writer.add_head_count(n_head)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "MPTModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class MPTModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.MPT\n    def set_vocab(self):\n        try:\n            self._set_vocab_gpt2()\n        except Exception:\n            # Fallback for SEA-LION model\n            self._set_vocab_sentencepiece()\n            self.gguf_writer.add_add_bos_token(False)\n            self.gguf_writer.add_pad_token_id(3)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "OrionModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class OrionModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.ORION\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n    def set_gguf_parameters(self):\n        head_count = self.hparams[\"num_attention_heads\"]\n        head_count_kv = self.hparams.get(\"num_key_value_heads\", head_count)\n        ctx_length = 0\n        if \"max_sequence_length\" in self.hparams:\n            ctx_length = self.hparams[\"max_sequence_length\"]",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "BaichuanModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class BaichuanModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.BAICHUAN\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_tensor_data_layout(\"Meta AI original pth\")\n        self.gguf_writer.add_rope_dimension_count(self.hparams[\"hidden_size\"] // self.hparams[\"num_attention_heads\"])\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        head_count = self.hparams[\"num_attention_heads\"]",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "XverseModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class XverseModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.XVERSE\n    def set_vocab(self):\n        assert (self.dir_model / \"tokenizer.json\").is_file()\n        dir_model = self.dir_model\n        hparams = self.hparams\n        tokens: list[bytes] = []\n        toktypes: list[int] = []\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(dir_model)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "FalconModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class FalconModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.FALCON\n    def set_gguf_parameters(self):\n        n_head = self.hparams.get(\"num_attention_heads\")\n        if n_head is None:\n            n_head = self.hparams[\"n_head\"]  # old name\n        n_head_kv = self.hparams.get(\"num_kv_heads\")\n        if n_head_kv is None:\n            n_head_kv = self.hparams.get(\"n_head_kv\", 1)  # old name\n        self.gguf_writer.add_context_length(2048)  # not in config.json",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "StarCoderModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class StarCoderModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.STARCODER\n    def set_gguf_parameters(self):\n        self.gguf_writer.add_context_length(self.hparams[\"n_positions\"])\n        self.gguf_writer.add_embedding_length(self.hparams[\"n_embd\"])\n        self.gguf_writer.add_feed_forward_length(4 * self.hparams[\"n_embd\"])\n        self.gguf_writer.add_block_count(self.block_count)\n        self.gguf_writer.add_head_count(self.hparams[\"n_head\"])\n        self.gguf_writer.add_head_count_kv(1)\n        self.gguf_writer.add_layer_norm_eps(self.hparams[\"layer_norm_epsilon\"])",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "RefactModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class RefactModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.REFACT\n    def set_vocab(self):\n        super().set_vocab()\n        # TODO: how to determine special FIM tokens automatically?\n        special_vocab = gguf.SpecialVocab(self.dir_model, load_merges=False,\n                                          special_token_types = ['prefix', 'suffix', 'middle', 'eot'])\n        special_vocab._set_special_token(\"prefix\", 1)\n        special_vocab._set_special_token(\"suffix\", 3)\n        special_vocab._set_special_token(\"middle\", 2)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "StableLMModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class StableLMModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.STABLELM\n    def set_vocab(self):\n        if (self.dir_model / \"tokenizer.json\").is_file():\n            self._set_vocab_gpt2()\n        else:\n            # StableLM 2 1.6B used to have a vocab in a similar format to Qwen's vocab\n            self._set_vocab_qwen()\n    def set_gguf_parameters(self):\n        hparams = self.hparams",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LlamaModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LlamaModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.LLAMA\n    undo_permute = True\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # fix for SmolVLM2, missing `num_attention_heads` in config.json\n        if self.hf_arch == \"VLlama3ForCausalLM\":\n            self.hparams[\"num_attention_heads\"] = self.hparams.get(\"num_attention_heads\", 32)\n        hparams = ModelBase.load_hparams(self.dir_model, is_mistral_format=False)\n        self.origin_hf_arch = hparams.get('architectures', [None])[0]",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ArceeModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ArceeModel(LlamaModel):\n    model_arch = gguf.MODEL_ARCH.ARCEE\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self._try_set_pooling_type()\n@ModelBase.register(\"AfmoeForCausalLM\")\nclass AfmoeModel(LlamaModel):\n    model_arch = gguf.MODEL_ARCH.AFMOE\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "AfmoeModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class AfmoeModel(LlamaModel):\n    model_arch = gguf.MODEL_ARCH.AFMOE\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        # MoE parameters\n        if (n_experts := self.hparams.get(\"num_experts\")) is not None:\n            self.gguf_writer.add_expert_count(n_experts)\n        if (n_shared_experts := self.hparams.get(\"num_shared_experts\")) is not None:\n            self.gguf_writer.add_expert_shared_count(n_shared_experts)\n        if (moe_intermediate_size := self.hparams.get(\"moe_intermediate_size\")) is not None:",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LlavaVisionModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LlavaVisionModel(MmprojModel):\n    img_break_tok_id = -1\n    use_break_tok = True\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if self.hparams.get(\"model_type\") == \"pixtral\":\n            # layer_norm_eps is not in config.json, it is hard-coded in modeling_pixtral.py\n            self.hparams[\"layer_norm_eps\"] = self.hparams.get(\"layer_norm_eps\", 1e-5)\n            if self.use_break_tok:\n                self.img_break_tok_id = self.get_token_id(\"[IMG_BREAK]\")",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "SmolVLMModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class SmolVLMModel(MmprojModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if self.hparams[\"model_type\"] == \"smolvlm_vision\":\n            # fix for SmolVLM2, missing some keys in config.json\n            # default values are taken from transformers code\n            self.hparams[\"hidden_size\"] = self.hparams.get(\"hidden_size\", 1152)\n            self.hparams[\"num_attention_heads\"] = self.hparams.get(\"num_attention_heads\", 16)\n            self.hparams[\"intermediate_size\"] = self.hparams.get(\"intermediate_size\", 3072)\n    def set_gguf_parameters(self):",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Llama4Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Llama4Model(LlamaModel):\n    model_arch = gguf.MODEL_ARCH.LLAMA4\n    undo_permute = False\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # IMPORTANT: the normal \"intermediate_size\" is renamed to \"intermediate_size_mlp\", we need to undo this\n        self.hparams[\"intermediate_size_moe\"] = self.hparams[\"intermediate_size\"]\n        self.hparams[\"intermediate_size\"] = self.hparams[\"intermediate_size_mlp\"]\n    def set_vocab(self):\n        self._set_vocab_gpt2()",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Llama4VisionModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Llama4VisionModel(MmprojModel):\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.LLAMA4)\n        self.gguf_writer.add_vision_attention_layernorm_eps(self.hparams[\"norm_eps\"])\n        self.gguf_writer.add_vision_projector_scale_factor(int(1.0 / self.hparams[\"pixel_shuffle_ratio\"]))\n        assert self.hparams[\"hidden_act\"] == \"gelu\"\n        self.gguf_writer.add_vision_use_gelu(True)\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        if \"multi_modal_projector\" in name or \"vision_model\" in name:",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Mistral3Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Mistral3Model(LlamaModel):\n    model_arch = gguf.MODEL_ARCH.MISTRAL3\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # for compatibility, we use LLAMA arch for older models\n        # TODO: remove this once everyone has migrated to newer version of llama.cpp\n        if self.hparams.get(\"model_type\") != \"ministral3\":\n            self.model_arch = gguf.MODEL_ARCH.LLAMA\n            self.gguf_writer.arch = gguf.MODEL_ARCH_NAMES[self.model_arch]\n            self.gguf_writer.add_architecture()",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "DeciModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class DeciModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.DECI\n    @staticmethod\n    def _ffn_mult_to_intermediate_size(ffn_mult: float, n_embd: int) -> int:\n        # DeciLM-specific code\n        intermediate_size = int(2 * ffn_mult * n_embd / 3)\n        return DeciModel._find_multiple(intermediate_size, 256)\n    @staticmethod\n    def _find_multiple(n: int, k: int) -> int:\n        # DeciLM-specific code",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "BitnetModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class BitnetModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.BITNET\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.LINEAR)\n        self.gguf_writer.add_rope_scaling_factor(1.0)\n    def weight_quant(self, weight: Tensor) -> Tensor:\n        dtype = weight.dtype",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "GrokModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class GrokModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.GROK\n    def set_vocab(self):\n        if (self.dir_model / 'tokenizer.model').is_file():\n            self._set_vocab_sentencepiece()\n            return\n        if not (self.dir_model / 'tokenizer.json').is_file() or not (self.dir_model / 'chat_template.jinja').is_file():\n            logger.error('Error: Missing vocab and chat template, download files from https://huggingface.co/alvarobartt/grok-2-tokenizer')\n            sys.exit(1)\n        self._set_vocab_gpt2()",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "DbrxModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class DbrxModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.DBRX\n    def set_gguf_parameters(self):\n        ffn_config = self.hparams[\"ffn_config\"]\n        attn_config = self.hparams[\"attn_config\"]\n        self.gguf_writer.add_block_count(self.block_count)\n        self.gguf_writer.add_context_length(self.hparams[\"max_seq_len\"])\n        self.gguf_writer.add_embedding_length(self.hparams[\"d_model\"])\n        self.gguf_writer.add_feed_forward_length(ffn_config[\"ffn_hidden_size\"])\n        self.gguf_writer.add_head_count(self.hparams[\"n_heads\"])",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "MiniCPMModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class MiniCPMModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.MINICPM\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        embedding_scale = float(self.hparams[\"scale_emb\"])\n        self.gguf_writer.add_embedding_scale(embedding_scale)\n        logger.info(f\"gguf: (minicpm) embedding_scale = {embedding_scale}\")\n        residual_scale = self.hparams[\"scale_depth\"] / self.hparams[\"num_hidden_layers\"] ** 0.5\n        self.gguf_writer.add_residual_scale(residual_scale)\n        logger.info(f\"gguf: (minicpm) residual_scale = {residual_scale}\")",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "MiniCPM3Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class MiniCPM3Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.MINICPM3\n    def set_gguf_parameters(self):\n        hparams = self.hparams\n        self.gguf_writer.add_file_type(self.ftype)\n        self.gguf_writer.add_context_length(hparams[\"max_position_embeddings\"])\n        self.gguf_writer.add_embedding_length(hparams[\"hidden_size\"])\n        self.gguf_writer.add_block_count(self.block_count)\n        self.gguf_writer.add_feed_forward_length(hparams[\"intermediate_size\"])\n        self.gguf_writer.add_head_count(hparams[\"num_attention_heads\"])",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "QwenModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class QwenModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.QWEN\n    @staticmethod\n    def token_bytes_to_string(b):\n        from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n        byte_encoder = bytes_to_unicode()\n        return ''.join([byte_encoder[ord(char)] for char in b.decode('latin-1')])\n    @staticmethod\n    def bpe(mergeable_ranks: dict[bytes, int], token: bytes, max_rank: int | None = None) -> list[bytes]:\n        parts = [bytes([b]) for b in token]",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Qwen2Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Qwen2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.QWEN2\n    def set_vocab(self):\n        try:\n            self._set_vocab_sentencepiece()\n        except FileNotFoundError:\n            self._set_vocab_gpt2()\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self._try_set_pooling_type()",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "DreamModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class DreamModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.DREAM\n    def get_vocab_base(self) -> tuple[list[str], list[int], str]:\n        tokens: list[str] = []\n        toktypes: list[int] = []\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(self.dir_model, trust_remote_code=True)\n        vocab_dict = tokenizer.get_vocab()\n        vocab_size = self.hparams.get(\"vocab_size\", len(vocab_dict))\n        assert max(vocab_dict.values()) < vocab_size",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LLaDAModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LLaDAModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.LLADA\n    undo_permute = True\n    def get_vocab_base(self) -> tuple[list[str], list[int], str]:\n        tokens: list[str] = []\n        toktypes: list[int] = []\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(self.dir_model, trust_remote_code=True)\n        vocab_dict = tokenizer.get_vocab()\n        vocab_size = self.hparams.get(\"vocab_size\", len(vocab_dict))",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Ernie4_5Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Ernie4_5Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.ERNIE4_5\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        num_heads = self.hparams[\"num_attention_heads\"]\n        num_kv_heads = self.hparams[\"num_key_value_heads\"]\n        if (head_dim := self.hparams.get(\"head_dim\")) is None:",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Ernie4_5MoeModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Ernie4_5MoeModel(Ernie4_5Model):\n    model_arch = gguf.MODEL_ARCH.ERNIE4_5_MOE\n    _experts: list[dict[str, Tensor]] | None = None\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._experts = [{} for _ in range(self.block_count)]\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_expert_count(self.hparams[\"moe_num_experts\"])\n        self.gguf_writer.add_expert_used_count(self.hparams[\"moe_k\"])",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Qwen2VLModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Qwen2VLModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.QWEN2VL\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n    def set_vocab(self):\n        try:\n            self._set_vocab_sentencepiece()\n        except FileNotFoundError:\n            self._set_vocab_gpt2()\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Qwen2VLVisionModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Qwen2VLVisionModel(MmprojModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert self.hparams_vision is not None\n        self.hparams_vision[\"image_size\"] = self.hparams_vision.get(\"image_size\", 560)\n        # rename config.json values\n        self.hparams_vision[\"num_attention_heads\"] = self.hparams_vision.get(\"num_heads\")\n        self.hparams_vision[\"num_hidden_layers\"] = self.hparams_vision.get(\"depth\")\n        if \"embed_dim\" in self.hparams_vision: # qwen2vl\n            self.hparams_vision[\"intermediate_size\"] = self.hparams_vision.get(\"hidden_size\")",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Qwen25OmniModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Qwen25OmniModel(Qwen2VLVisionModel):\n    has_vision_encoder = True\n    has_audio_encoder = True\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert self.hparams_audio is not None\n        self.hparams_audio[\"hidden_size\"] = self.hparams_audio[\"d_model\"]\n        self.hparams_audio[\"intermediate_size\"] = self.hparams_audio[\"encoder_ffn_dim\"]\n        self.hparams_audio[\"num_attention_heads\"] = self.hparams_audio[\"encoder_attention_heads\"]\n    def set_gguf_parameters(self):",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "InternVisionModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class InternVisionModel(MmprojModel):\n    def set_gguf_parameters(self):\n        assert self.hparams_vision is not None\n        if isinstance(self.hparams_vision['image_size'], list):\n            self.hparams_vision['image_size'] = self.hparams_vision['image_size'][0]\n        if isinstance(self.hparams_vision['patch_size'], list):\n            self.hparams_vision['patch_size'] = self.hparams_vision['patch_size'][0]\n        super().set_gguf_parameters()\n        hparams = self.hparams\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.INTERNVL)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "WavTokenizerDecModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class WavTokenizerDecModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.WAVTOKENIZER_DEC\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        if \\\n                name.endswith(\"codebook.cluster_size\") or \\\n                name.endswith(\"codebook.embed_avg\") or \\\n                name.endswith(\"codebook.inited\"):\n            logger.debug(f\"Skipping {name!r}\")\n            return\n        logger.info(f\"{self.map_tensor_name(name)} -> {data_torch.shape}\")",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Qwen2MoeModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Qwen2MoeModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.QWEN2MOE\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        if (n_experts := self.hparams.get(\"num_experts\")) is not None:\n            self.gguf_writer.add_expert_count(n_experts)\n        if (moe_intermediate_size := self.hparams.get(\"moe_intermediate_size\")) is not None:\n            self.gguf_writer.add_expert_feed_forward_length(moe_intermediate_size)\n            logger.info(f\"gguf: expert feed forward length = {moe_intermediate_size}\")\n        if (shared_expert_intermediate_size := self.hparams.get('shared_expert_intermediate_size')) is not None:",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Qwen3Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Qwen3Model(Qwen2Model):\n    model_arch = gguf.MODEL_ARCH.QWEN3\n    # extra logic for rerank models\n    is_rerank: bool = False\n    is_tied_embeddings: bool = False\n    token_false_id: int | None = None\n    token_true_id: int | None = None\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # track for intern-s1-mini",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Qwen3MoeModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Qwen3MoeModel(Qwen2MoeModel):\n    model_arch = gguf.MODEL_ARCH.QWEN3MOE\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        hparams = ModelBase.load_hparams(self.dir_model, False)\n        self.origin_hf_arch = hparams.get('architectures', [None])[0]\n    def set_vocab(self):\n        # deal with intern-s1\n        if self.origin_hf_arch == 'InternS1ForConditionalGeneration':\n            self._set_vocab_interns1()",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Qwen3NextModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Qwen3NextModel(Qwen2MoeModel):\n    model_arch = gguf.MODEL_ARCH.QWEN3NEXT\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_ssm_conv_kernel(self.hparams[\"linear_conv_kernel_dim\"])\n        self.gguf_writer.add_ssm_state_size(self.hparams[\"linear_key_head_dim\"])\n        self.gguf_writer.add_ssm_group_count(self.hparams[\"linear_num_key_heads\"])\n        self.gguf_writer.add_ssm_time_step_rank(self.hparams[\"linear_num_value_heads\"])\n        self.gguf_writer.add_ssm_inner_size(self.hparams[\"linear_value_head_dim\"] * self.hparams[\"linear_num_value_heads\"])\n        if (rope_dim := self.hparams.get(\"head_dim\")) is None:",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "RND1Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class RND1Model(Qwen2MoeModel):\n    model_arch = gguf.MODEL_ARCH.RND1\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        # RND1 specific parameters\n        # RND1 uses bidirectional attention\n        self.gguf_writer.add_causal_attention(False)\n        if (mask_token_id := self.hparams.get(\"mask_token_id\")) is not None:\n            self.gguf_writer.add_mask_token_id(mask_token_id)\n@ModelBase.register(\"Qwen3VLForConditionalGeneration\", \"Qwen3VLMoeForConditionalGeneration\")",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Qwen3VLVisionModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Qwen3VLVisionModel(MmprojModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert self.hparams_vision is not None\n        # Compute image_size if not present\n        if \"image_size\" not in self.hparams_vision:\n            # For Qwen3VL/Qwen3VLMoe, compute from num_position_embeddings\n            num_pos = self.hparams_vision.get(\"num_position_embeddings\", 2304)\n            patch_size = self.hparams_vision.get(\"patch_size\", 16)\n            # num_position_embeddings = (image_size / patch_size) ** 2",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Glm4VVisionModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Glm4VVisionModel(Qwen3VLVisionModel):\n    def set_gguf_parameters(self):\n        MmprojModel.set_gguf_parameters(self) # skip Qwen3VLVisionModel parameters\n        assert self.hparams_vision is not None\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.GLM4V)\n        hidden_act = str(self.hparams_vision.get(\"hidden_act\", \"\")).lower()\n        if hidden_act == \"gelu\":\n            self.gguf_writer.add_vision_use_gelu(True)\n        elif hidden_act == \"silu\":\n            self.gguf_writer.add_vision_use_silu(True)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Qwen3VLTextModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Qwen3VLTextModel(Qwen3Model):\n    model_arch = gguf.MODEL_ARCH.QWEN3VL\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        # Handle MRoPE (Multi-axis Rotary Position Embedding) for Qwen3-VL\n        vision_config = self.hparams.get(\"vision_config\", {})\n        deepstack_layer_num = len(vision_config.get(\"deepstack_visual_indexes\", []))\n        self.gguf_writer.add_num_deepstack_layers(deepstack_layer_num)\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        # Skip vision tensors - they go in the mmproj file",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Qwen3VLMoeTextModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Qwen3VLMoeTextModel(Qwen3MoeModel):\n    model_arch = gguf.MODEL_ARCH.QWEN3VLMOE\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        vision_config = self.hparams.get(\"vision_config\", {})\n        deepstack_layer_num = len(vision_config.get(\"deepstack_visual_indexes\", []))\n        self.gguf_writer.add_num_deepstack_layers(deepstack_layer_num)\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        # Skip vision tensors - they go in the mmproj file\n        if name.startswith(\"model.visual.\"):",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "GPT2Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class GPT2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.GPT2\n    def set_gguf_parameters(self):\n        self.gguf_writer.add_block_count(self.block_count)\n        self.gguf_writer.add_context_length(self.hparams[\"n_ctx\"])\n        self.gguf_writer.add_embedding_length(self.hparams[\"n_embd\"])\n        self.gguf_writer.add_feed_forward_length(4 * self.hparams[\"n_embd\"])\n        self.gguf_writer.add_head_count(self.hparams[\"n_head\"])\n        self.gguf_writer.add_layer_norm_eps(self.hparams[\"layer_norm_epsilon\"])\n        self.gguf_writer.add_file_type(self.ftype)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Phi2Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Phi2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.PHI2\n    def set_gguf_parameters(self):\n        rot_pct = self.find_hparam([\"partial_rotary_factor\"])\n        n_embd = self.find_hparam([\"hidden_size\", \"n_embd\"])\n        n_head = self.find_hparam([\"num_attention_heads\", \"n_head\"])\n        self.gguf_writer.add_context_length(self.find_hparam([\"n_positions\", \"max_position_embeddings\"]))\n        self.gguf_writer.add_embedding_length(n_embd)\n        self.gguf_writer.add_feed_forward_length(4 * n_embd)\n        self.gguf_writer.add_block_count(self.block_count)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Phi3MiniModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Phi3MiniModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.PHI3\n    def set_vocab(self):\n        # Phi-4 model uses GPT2Tokenizer\n        tokenizer_config_file = self.dir_model / 'tokenizer_config.json'\n        if tokenizer_config_file.is_file():\n            with open(tokenizer_config_file, \"r\", encoding=\"utf-8\") as f:\n                tokenizer_config_json = json.load(f)\n                tokenizer_class = tokenizer_config_json['tokenizer_class']\n                if tokenizer_class == 'GPT2Tokenizer':",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "PhiMoeModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class PhiMoeModel(Phi3MiniModel):\n    model_arch = gguf.MODEL_ARCH.PHIMOE\n    _experts: list[dict[str, Tensor]] | None = None\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_expert_used_count(self.hparams[\"num_experts_per_tok\"])\n        self.gguf_writer.add_expert_count(self.hparams[\"num_local_experts\"])\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        # process the experts separately\n        if name.find(\"block_sparse_moe.experts\") != -1:",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "PlamoModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class PlamoModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.PLAMO\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n    def set_gguf_parameters(self):\n        hparams = self.hparams\n        self.gguf_writer.add_context_length(4096)  # not in config.json\n        self.gguf_writer.add_embedding_length(hparams[\"hidden_size\"])\n        self.gguf_writer.add_feed_forward_length(hparams[\"intermediate_size\"])\n        self.gguf_writer.add_block_count(self.block_count)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Plamo2Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Plamo2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.PLAMO2\n    def set_vocab(self):\n        self._set_vocab_plamo()\n    def set_gguf_parameters(self):\n        hparams = self.hparams\n        self.gguf_writer.add_vocab_size(self.hparams[\"vocab_size\"])\n        # Which layers are Mamba layers\n        # PLaMo 2 uses mamba_step to indicate the pattern (e.g., 2 means every other layer)\n        # This logic matches modeling_plamo.py's is_mamba function",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Plamo3Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Plamo3Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.PLAMO3\n    def set_vocab(self):\n        self._set_vocab_plamo()\n        tokenizer_config_path = self.dir_model / \"tokenizer_config.json\"\n        tokenizer_config = {}\n        if tokenizer_config_path.is_file():\n            with open(tokenizer_config_path, encoding=\"utf-8\") as f:\n                tokenizer_config = json.load(f)\n        chat_template = tokenizer_config.get(\"chat_template\")",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "CodeShellModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class CodeShellModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.CODESHELL\n    def set_gguf_parameters(self):\n        self.gguf_writer.add_context_length(self.hparams[\"n_positions\"])\n        self.gguf_writer.add_embedding_length(self.hparams[\"n_embd\"])\n        self.gguf_writer.add_feed_forward_length(4 * self.hparams[\"n_embd\"])\n        self.gguf_writer.add_block_count(self.block_count)\n        self.gguf_writer.add_head_count(self.hparams[\"n_head\"])\n        self.gguf_writer.add_head_count_kv(self.hparams[\"num_query_groups\"])\n        self.gguf_writer.add_layer_norm_eps(self.hparams[\"layer_norm_epsilon\"])",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "InternLM2Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class InternLM2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.INTERNLM2\n    def set_vocab(self):\n        # (TODO): Is there a better way?\n        # Copy from _set_vocab_sentencepiece, The only difference is that we will treat the character\n        # \\x00 specially and convert it into an emoji character to prevent it from being mistakenly\n        # recognized as an empty string in C++.\n        from sentencepiece import SentencePieceProcessor\n        from sentencepiece import sentencepiece_model_pb2 as model\n        tokenizer_path = self.dir_model / 'tokenizer.model'",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "InternLM3Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class InternLM3Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.LLAMA\n    def set_vocab(self):\n        tokens, scores, toktypes = self._create_vocab_sentencepiece()\n        self.gguf_writer.add_tokenizer_model(\"llama\")\n        self.gguf_writer.add_tokenizer_pre(\"default\")\n        self.gguf_writer.add_token_list(tokens)\n        self.gguf_writer.add_token_scores(scores)\n        self.gguf_writer.add_token_types(toktypes)\n        special_vocab = gguf.SpecialVocab(self.dir_model, n_vocab=len(tokens))",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "BertModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class BertModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.BERT\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.vocab_size = None\n        if cls_out_labels := self.hparams.get(\"id2label\"):\n            if len(cls_out_labels) == 2 and cls_out_labels[0] == \"LABEL_0\":\n                # Remove dummy labels added by AutoConfig\n                cls_out_labels = None\n        self.cls_out_labels = cls_out_labels",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "DistilBertModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class DistilBertModel(BertModel):\n    model_arch = gguf.MODEL_ARCH.BERT\n    def set_gguf_parameters(self):\n        self.gguf_writer.add_layer_norm_eps(1e-12)\n        logger.info(\"gguf: layer norm epsilon = 1e-12\")\n        super().set_gguf_parameters()\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        if name.startswith(\"distilbert.\"):\n            name = name[11:]\n        # These layers act as MLM head, so we don't need them",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "RobertaModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class RobertaModel(BertModel):\n    model_arch = gguf.MODEL_ARCH.BERT\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # we need the pad_token_id to know how to chop down position_embd matrix\n        if (pad_token_id := self.hparams.get(\"pad_token_id\")) is not None:\n            self._position_offset = 1 + pad_token_id\n            if \"max_position_embeddings\" in self.hparams:\n                self.hparams[\"max_position_embeddings\"] -= self._position_offset\n        else:",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "NomicBertModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class NomicBertModel(BertModel):\n    model_arch = gguf.MODEL_ARCH.BERT\n    def __init__(self, dir_model: Path, ftype: gguf.LlamaFileType, fname_out: Path, **kwargs: Any):\n        hparams = kwargs.pop(\"hparams\", None)\n        if hparams is None:\n            hparams = ModelBase.load_hparams(dir_model, False)\n        self.is_moe = bool(hparams.get(\"moe_every_n_layers\"))\n        self.model_arch = gguf.MODEL_ARCH.NOMIC_BERT_MOE if self.is_moe else gguf.MODEL_ARCH.NOMIC_BERT\n        super().__init__(dir_model, ftype, fname_out, hparams=hparams, **kwargs)\n        self._tokenizer_is_xlmroberta = self._is_tokenizer_xlmroberta()",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "NeoBert",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class NeoBert(BertModel):\n    model_arch = gguf.MODEL_ARCH.NEO_BERT\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        # NeoBERT uses 2/3 of the intermediate size as feed forward length\n        self.gguf_writer.add_feed_forward_length(int(2 * self.hparams[\"intermediate_size\"] / 3))\n        self.gguf_writer.add_rope_freq_base(10000.0)  # default value for NeoBERT\n        self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.NONE)\n        f_rms_eps = self.hparams.get(\"norm_eps\", 1e-6)  # default value for NeoBERT\n        self.gguf_writer.add_layer_norm_rms_eps(f_rms_eps)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "XLMRobertaModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class XLMRobertaModel(BertModel):\n    model_arch = gguf.MODEL_ARCH.BERT\n    _lora_files = {}\n    _lora_names = []\n    def __init__(self, dir_model: Path, ftype: gguf.LlamaFileType, fname_out: Path, **kwargs: Any):\n        hparams = kwargs.pop(\"hparams\", None)\n        if hparams is None:\n            hparams = ModelBase.load_hparams(dir_model, False)\n        if lora_names := hparams.get(\"lora_adaptations\"):\n            self._lora_names = lora_names",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "GemmaModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class GemmaModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.GEMMA\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n        # TODO: these special tokens should be exported only for the CodeGemma family\n        special_vocab = gguf.SpecialVocab(self.dir_model, load_merges=False,\n                                          special_token_types = ['prefix', 'suffix', 'middle', 'fsep', 'eot'])\n        special_vocab._set_special_token(\"prefix\", 67)\n        special_vocab._set_special_token(\"suffix\", 69)\n        special_vocab._set_special_token(\"middle\", 68)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Gemma2Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Gemma2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.GEMMA2\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n        self.gguf_writer.add_add_space_prefix(False)\n    def set_gguf_parameters(self):\n        hparams = self.hparams\n        self.gguf_writer.add_context_length(hparams[\"max_position_embeddings\"])\n        self.gguf_writer.add_embedding_length(hparams[\"hidden_size\"])\n        self.gguf_writer.add_block_count(self.block_count)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Gemma3Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Gemma3Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.GEMMA3\n    norm_shift = 1.0  # Gemma3RMSNorm adds 1.0 to the norm value\n    def set_vocab(self):\n        if (self.dir_model / \"tokenizer.model\").is_file():\n            self._set_vocab_sentencepiece()\n            self.gguf_writer.add_add_space_prefix(False)\n        else:\n            self._set_vocab_gpt2()\n    def set_gguf_parameters(self):",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "EmbeddingGemma",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class EmbeddingGemma(Gemma3Model):\n    model_arch = gguf.MODEL_ARCH.GEMMA_EMBEDDING\n    module_paths = []\n    dense_features_dims = {}\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if self.sentence_transformers_dense_modules:\n            # read modules.json to determine if model has Dense layers\n            modules_file = self.dir_model / \"modules.json\"\n            if modules_file.is_file():",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Gemma3VisionModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Gemma3VisionModel(MmprojModel):\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        hparams = self.hparams\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.GEMMA3)\n        # default values below are taken from HF tranformers code\n        self.gguf_writer.add_vision_attention_layernorm_eps(hparams.get(\"layer_norm_eps\", 1e-6))\n        self.gguf_writer.add_vision_use_gelu(True)\n        # calculate proj_scale_factor (used by tinygemma3 test model)\n        image_seq_length = self.preprocessor_config.get(\"image_seq_length\", 256)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ConformerAudioModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ConformerAudioModel(MmprojModel):\n    _batch_norm_tensors: list[dict[str, Tensor]] | None = None\n    @staticmethod\n    def is_audio_tensor(name: str):\n        return any(p in name for p in [\"audio\", \"codebook\", \"conformer\", \"depth_embedding\", \"depthformer\", \"depth_linear\"])\n    def tensor_force_quant(self, name, new_name, bid, n_dims):\n        if ConformerAudioModel.is_audio_tensor(name):\n            if \".conv\" in name or \"_conv\" in name and \".weight\" in name:\n                return gguf.GGMLQuantizationType.F32\n        return super().tensor_force_quant(name, new_name, bid, n_dims)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Gemma3nVisionAudioModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Gemma3nVisionAudioModel(ConformerAudioModel):\n    has_audio_encoder = True\n    has_vision_encoder = True\n    # Double indexed mapping for MobileNetV5 blocks (not supported by tensor_mapping.py)\n    # This is the only known model having this, so we prefer implementing it outside of tensor_mapping.py\n    block_tensor_mapping = {\n        \"model.vision_tower.timm_model.blocks.{bid}.{sid}.conv_exp.weight\":             \"v.blk.{bid}.{sid}.conv_exp.weight\",\n        \"model.vision_tower.timm_model.blocks.{bid}.{sid}.bn1.weight\":                  \"v.blk.{bid}.{sid}.bn1.weight\",\n        \"model.vision_tower.timm_model.blocks.{bid}.{sid}.conv_pwl.weight\":             \"v.blk.{bid}.{sid}.conv_pwl.weight\",\n        \"model.vision_tower.timm_model.blocks.{bid}.{sid}.bn2.weight\":                  \"v.blk.{bid}.{sid}.bn2.weight\",",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Gemma3NModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Gemma3NModel(Gemma3Model):\n    model_arch = gguf.MODEL_ARCH.GEMMA3N\n    norm_shift = 0.0 # same value with Gemma3p5RMSNorm scale_shift on python code\n    _altup_proj: list[Tensor] = []\n    _altup_unembd: list[Tensor] = []\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert self.hparams[\"altup_num_inputs\"] == 4, \"Current conversion only supports 4 altup inputs\"\n        self._altup_proj = [\n            torch.Tensor(), # to be replaced",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "StarCoder2Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class StarCoder2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.STARCODER2\n@ModelBase.register(\"Rwkv6ForCausalLM\")\nclass Rwkv6Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.RWKV6\n    def set_vocab(self):\n        self._set_vocab_rwkv_world()\n    def set_gguf_parameters(self):\n        head_size = self.hparams[\"head_size\"]\n        hidden_size = self.hparams[\"hidden_size\"]",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Rwkv6Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Rwkv6Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.RWKV6\n    def set_vocab(self):\n        self._set_vocab_rwkv_world()\n    def set_gguf_parameters(self):\n        head_size = self.hparams[\"head_size\"]\n        hidden_size = self.hparams[\"hidden_size\"]\n        layer_norm_eps = self.hparams[\"layer_norm_epsilon\"]\n        rescale_every_n_layers = self.hparams[\"rescale_every\"]\n        intermediate_size = self.hparams[\"intermediate_size\"] if self.hparams[\"intermediate_size\"] is not None else int((hidden_size * 3.5) // 32 * 32)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "RWKV6Qwen2Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class RWKV6Qwen2Model(Rwkv6Model):\n    model_arch = gguf.MODEL_ARCH.RWKV6QWEN2\n    def set_vocab(self):\n        try:\n            self._set_vocab_sentencepiece()\n        except FileNotFoundError:\n            self._set_vocab_gpt2()\n    def set_gguf_parameters(self):\n        num_attention_heads = self.hparams[\"num_attention_heads\"]\n        num_key_value_heads = self.hparams[\"num_key_value_heads\"]",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Rwkv7Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Rwkv7Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.RWKV7\n    def set_vocab(self):\n        self._set_vocab_rwkv_world()\n    def calc_lora_rank(self, hidden_size, exponent, multiplier):\n        return max(1, round(hidden_size ** exponent * multiplier / 32)) * 32\n    def set_gguf_parameters(self):\n        try:\n            head_size = self.hparams[\"head_size\"]\n            layer_norm_eps = self.hparams[\"layer_norm_epsilon\"]",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ARwkv7Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ARwkv7Model(Rwkv7Model):\n    model_arch = gguf.MODEL_ARCH.ARWKV7\n    def set_vocab(self):\n        try:\n            self._set_vocab_sentencepiece()\n        except FileNotFoundError:\n            self._set_vocab_gpt2()\n    def set_gguf_parameters(self):\n        hidden_size = self.hparams[\"hidden_size\"]\n        head_size = self.hparams[\"head_size\"]",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "MaincoderModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class MaincoderModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.MAINCODER\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        if (head_dim := self.hparams.get(\"head_dim\")) is not None:\n            self.gguf_writer.add_rope_dimension_count(head_dim)\n@ModelBase.register(\"MambaForCausalLM\", \"MambaLMHeadModel\", \"FalconMambaForCausalLM\")\nclass MambaModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.MAMBA\n    def __init__(self, dir_model: Path, *args, **kwargs):",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "MambaModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class MambaModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.MAMBA\n    def __init__(self, dir_model: Path, *args, **kwargs):\n        # Avoid using AutoConfig for hparams\n        hparams = kwargs.pop(\"hparams\", None)\n        if hparams is None:\n            with open(dir_model / \"config.json\", \"r\", encoding=\"utf-8\") as f:\n                hparams = json.load(f)\n        super().__init__(dir_model, *args, hparams=hparams, **kwargs)\n    def set_vocab(self):",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Mamba2Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Mamba2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.MAMBA2\n    def __init__(self, dir_model: Path, *args, **kwargs):\n        # Avoid using AutoConfig for hparams\n        # It wrongly assumes all Mamba2 models are Mamba-Codestral-7B-v0.1\n        hparams = kwargs.pop(\"hparams\", None)\n        if hparams is None:\n            with open(dir_model / \"config.json\", \"r\", encoding=\"utf-8\") as f:\n                hparams = json.load(f)\n        super().__init__(dir_model, *args, hparams=hparams, **kwargs)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "JambaModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class JambaModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.JAMBA\n    def set_vocab(self):\n        if (self.dir_model / \"tokenizer.model\").is_file():\n            self._set_vocab_sentencepiece()\n        else:\n            self._set_vocab_llama_hf()\n            self.gguf_writer.add_add_space_prefix(False)\n    def set_gguf_parameters(self):\n        d_model = self.find_hparam([\"hidden_size\", \"mamba_d_model\"])",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "CommandR2Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class CommandR2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.COMMAND_R\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # max_position_embeddings = 8192 in config.json but model was actually\n        # trained on 128k context length\n        # aya-23 models don't have model_max_length specified\n        self.hparams[\"max_position_embeddings\"] = self.find_hparam([\"model_max_length\", \"max_position_embeddings\"])\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Cohere2Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Cohere2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.COHERE2\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_logit_scale(self.hparams[\"logit_scale\"])\n        self.gguf_writer.add_sliding_window(self.hparams[\"sliding_window\"])\n        self.gguf_writer.add_vocab_size(self.hparams[\"vocab_size\"])\n        rotary_pct = self.hparams[\"rotary_pct\"]\n        hidden_size = self.hparams[\"hidden_size\"]\n        num_attention_heads = self.hparams[\"num_attention_heads\"]",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "OlmoModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class OlmoModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.OLMO\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_layer_norm_eps(1e-5)\n        clip_qkv = self.hparams.get(\"clip_qkv\")\n        if clip_qkv is not None:\n            self.gguf_writer.add_clamp_kqv(clip_qkv)\n    # Same as super class, but permuting q_proj, k_proj\n    # Copied from: LlamaModel",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "SeedOssModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class SeedOssModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.SEED_OSS\n@ModelBase.register(\"Olmo2ForCausalLM\")\n@ModelBase.register(\"Olmo3ForCausalLM\")\nclass Olmo2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.OLMO2\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        if \"sliding_window\" in self.hparams:\n            self.gguf_writer.add_sliding_window(self.hparams[\"sliding_window\"])",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Olmo2Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Olmo2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.OLMO2\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        if \"sliding_window\" in self.hparams:\n            self.gguf_writer.add_sliding_window(self.hparams[\"sliding_window\"])\n            sliding_window_pattern = []\n            if \"layer_types\" in self.hparams:\n                sliding_window_pattern = [t == \"sliding_attention\" for t in self.hparams[\"layer_types\"]]\n            else:",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "OlmoeModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class OlmoeModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.OLMOE\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_layer_norm_rms_eps(1e-5)\n        if (n_experts := self.hparams.get(\"num_experts\")) is not None:\n            self.gguf_writer.add_expert_count(n_experts)\n    _experts: list[dict[str, Tensor]] | None = None\n    # Copied from: Qwen2MoeModel\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "JinaBertV2Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class JinaBertV2Model(BertModel):\n    model_arch = gguf.MODEL_ARCH.JINA_BERT_V2\n    def set_vocab(self):\n        tokenizer_class = 'BertTokenizer'\n        with open(self.dir_model / \"tokenizer_config.json\", \"r\", encoding=\"utf-8\") as f:\n            tokenizer_class = json.load(f)['tokenizer_class']\n        if tokenizer_class == 'BertTokenizer':\n            super().set_vocab()\n        elif tokenizer_class == 'RobertaTokenizer':\n            self._set_vocab_gpt2()",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "OpenELMModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class OpenELMModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.OPENELM\n    @staticmethod\n    def _make_divisible(v: float | int, divisor: int) -> int:\n        # ref: https://huggingface.co/apple/OpenELM-270M-Instruct/blob/eb111ff2e6724348e5b905984063d4064d4bc579/configuration_openelm.py#L34-L38\n        new_v = max(divisor, int(v + divisor / 2) // divisor * divisor)\n        # Make sure that round down does not go down by more than 10%.\n        if new_v < 0.9 * v:\n            new_v += divisor\n        return new_v",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ArcticModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ArcticModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.ARCTIC\n    def set_vocab(self):\n        # The reason for using a custom implementation here is that the\n        # snowflake-arctic-instruct model redefined tokens 31998 and 31999 from\n        # tokenizer.model and used them as BOS and EOS instead of adding new tokens.\n        from sentencepiece import SentencePieceProcessor\n        tokenizer_path = self.dir_model / 'tokenizer.model'\n        if not tokenizer_path.is_file():\n            logger.error(f'Error: Missing {tokenizer_path}')",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "DeepseekModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class DeepseekModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.DEEPSEEK\n    def set_vocab(self):\n        try:\n            self._set_vocab_sentencepiece()\n        except FileNotFoundError:\n            self._set_vocab_gpt2()\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        hparams = self.hparams",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "DeepseekV2Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class DeepseekV2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.DEEPSEEK2\n    def set_vocab(self):\n        try:\n            self._set_vocab_gpt2()\n            return\n        except Exception:\n            pass\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(self.dir_model, trust_remote_code=True)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "MiniMaxM2Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class MiniMaxM2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.MINIMAXM2\n    _experts_cache: dict[int, dict[str, Tensor]] = {}\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.hparams[\"num_experts\"] = self.hparams[\"num_local_experts\"]\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_expert_feed_forward_length(self.find_hparam([\"intermediate_size\"]))\n        self.gguf_writer.add_rope_dimension_count(self.find_hparam([\"rotary_dim\"]))",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "MimoV2Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class MimoV2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.MIMO2\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        assert self.hparams[\"swa_head_dim\"] == self.hparams[\"head_dim\"]\n        assert self.hparams[\"swa_num_attention_heads\"] == self.hparams[\"num_attention_heads\"]\n        assert self.hparams[\"swa_v_head_dim\"] == self.hparams[\"v_head_dim\"]\n        assert self.hparams[\"topk_method\"] == \"noaux_tc\"\n        n_head_kv = self.hparams[\"num_key_value_heads\"]\n        n_head_kv_swa = self.hparams[\"swa_num_key_value_heads\"]",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "PanguEmbeddedModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class PanguEmbeddedModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.PANGU_EMBED\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n        tokenizer_config_file = self.dir_model / 'tokenizer_config.json'\n        if tokenizer_config_file.is_file():\n            with open(tokenizer_config_file, \"r\", encoding=\"utf-8\") as f:\n                tokenizer_config_json = json.load(f)\n                if \"add_prefix_space\" in tokenizer_config_json:\n                    self.gguf_writer.add_add_space_prefix(tokenizer_config_json[\"add_prefix_space\"])",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Dots1Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Dots1Model(Qwen2MoeModel):\n    model_arch = gguf.MODEL_ARCH.DOTS1\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.hparams[\"num_experts\"] = self.hparams[\"n_routed_experts\"]\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_leading_dense_block_count(self.hparams[\"first_k_dense_replace\"])\n        self.gguf_writer.add_expert_shared_count(self.hparams[\"n_shared_experts\"])\n        self.gguf_writer.add_expert_weights_scale(self.hparams[\"routed_scaling_factor\"])",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "PLMModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class PLMModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.PLM\n    def set_vocab(self):\n        self._set_vocab_gpt2()\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        hparams = self.hparams\n        self.gguf_writer.add_vocab_size(hparams[\"vocab_size\"])\n        self.gguf_writer.add_kv_lora_rank(hparams[\"kv_lora_rank\"])\n        self.gguf_writer.add_key_length(hparams[\"qk_nope_head_dim\"] + hparams[\"qk_rope_head_dim\"])",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "T5Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class T5Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.T5\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.shared_token_embeddings_found = False\n    def set_vocab(self):\n        # to avoid TypeError: Descriptors cannot be created directly\n        # exception when importing sentencepiece_model_pb2\n        os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n        from sentencepiece import SentencePieceProcessor",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "T5EncoderModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class T5EncoderModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.T5ENCODER\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.shared_token_embeddings_found = False\n    def set_vocab(self):\n        # to avoid TypeError: Descriptors cannot be created directly\n        # exception when importing sentencepiece_model_pb2\n        os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n        from sentencepiece import SentencePieceProcessor",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "JaisModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class JaisModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.JAIS\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # SwigLU activation\n        assert self.hparams[\"activation_function\"] == \"swiglu\"\n        # ALiBi position embedding\n        assert self.hparams[\"position_embedding_type\"] == \"alibi\"\n        # Embeddings scale\n        self.embeddings_scale = 1.0",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Glm4Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Glm4Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.GLM4\n    use_mrope = False\n    partial_rotary_factor = 0.5\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.partial_rotary_factor = self.rope_parameters.get(\"partial_rotary_factor\", 0.5)\n        if \"mrope_section\" in self.rope_parameters:\n            self.use_mrope = True\n            logger.info(\"Q/K weight will need to be permuted for M-RoPE\")",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Glm4MoeModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Glm4MoeModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.GLM4_MOE\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # GLM4_MOE has num_hidden_layers + 1 actual layers (including NextN layer)\n        self.block_count = self.hparams[\"num_hidden_layers\"] + self.hparams.get(\"num_nextn_predict_layers\", 0)\n        self.tensor_map = gguf.get_tensor_name_map(self.model_arch, self.block_count)\n    def set_vocab(self):\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(self.dir_model)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Glm4MoeLiteModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Glm4MoeLiteModel(DeepseekV2Model):\n    model_arch = gguf.MODEL_ARCH.DEEPSEEK2\n    # copied from Glm4MoeModel\n    def set_vocab(self):\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(self.dir_model)\n        special_vocab = gguf.SpecialVocab(self.dir_model, load_merges=True)\n        tokens, toktypes, tokpre = self.get_vocab_base()\n        self.gguf_writer.add_tokenizer_model(\"gpt2\")\n        self.gguf_writer.add_tokenizer_pre(tokpre)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ChatGLMModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ChatGLMModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.CHATGLM\n    def set_vocab_chatglm3(self):\n        dir_model = self.dir_model\n        hparams = self.hparams\n        tokens: list[bytes] = []\n        toktypes: list[int] = []\n        scores: list[float] = []\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(dir_model, trust_remote_code=True)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "NemotronModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class NemotronModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.NEMOTRON\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n        self.gguf_writer.add_pad_token_id(0)\n        self.gguf_writer.add_unk_token_id(1)\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        hparams = self.hparams\n        self.gguf_writer.add_vocab_size(hparams[\"vocab_size\"])",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ExaoneModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ExaoneModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.EXAONE\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        hparams = self.hparams\n        assert (hparams[\"activation_function\"] == \"silu\")\n        rotary_factor = self.find_hparam([\"partial_rotary_factor\", \"rope_pct\"], optional=True)\n        rotary_factor = rotary_factor if rotary_factor is not None else 1.0\n        self.gguf_writer.add_rope_dimension_count(int(rotary_factor * (hparams[\"hidden_size\"] // hparams[\"num_attention_heads\"])))\n    def generate_extra_tensors(self) -> Iterable[tuple[str, Tensor]]:",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "Exaone4Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class Exaone4Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.EXAONE4\n    def set_vocab(self):\n        tokens, toktypes, tokpre = self.get_vocab_base()\n        self.gguf_writer.add_tokenizer_model(\"gpt2\")\n        self.gguf_writer.add_tokenizer_pre(tokpre)\n        self.gguf_writer.add_token_list(tokens)\n        self.gguf_writer.add_token_types(toktypes)\n        special_vocab = gguf.SpecialVocab(self.dir_model, load_merges=True)\n        special_vocab.add_to_gguf(self.gguf_writer)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ExaoneMoEModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ExaoneMoEModel(Exaone4Model):\n    model_arch = gguf.MODEL_ARCH.EXAONE_MOE\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.block_count = self.hparams[\"num_hidden_layers\"] + self.hparams.get(\"num_nextn_predict_layers\", 0)\n        self.tensor_map = gguf.get_tensor_name_map(self.model_arch, self.block_count)\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_expert_count(self.hparams[\"num_experts\"])\n        moe_intermediate_size = self.hparams[\"moe_intermediate_size\"]",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "GraniteModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class GraniteModel(LlamaModel):\n    \"\"\"Conversion for IBM's GraniteForCausalLM\"\"\"\n    model_arch = gguf.MODEL_ARCH.GRANITE\n    def set_gguf_parameters(self):\n        \"\"\"Granite uses standard llama parameters with the following differences:\n        - No head_dim support\n        - New multiplier params:\n            - attention_scale\n            - embedding_scale\n            - residual_scale",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "GraniteMoeModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class GraniteMoeModel(GraniteModel):\n    \"\"\"Conversion for IBM's GraniteMoeForCausalLM\"\"\"\n    model_arch = gguf.MODEL_ARCH.GRANITE_MOE\n    def set_gguf_parameters(self):\n        \"\"\"GraniteMoeShared uses GraniteMoe parameters plus the following:\n        - shared_intermediate_size\n        \"\"\"\n        super().set_gguf_parameters()\n        if shared_feed_forward_length := self.hparams.get(\"shared_intermediate_size\"):\n            self.gguf_writer.add_expert_shared_feed_forward_length(shared_feed_forward_length)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "GraniteHybridModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class GraniteHybridModel(Mamba2Model, GraniteMoeModel):\n    \"\"\"GraniteHybrid is a hybrid SSM + Attention model that uses Mamba2 SSM\n    layers and optionally uses MoE w/ a shared expert\"\"\"\n    model_arch = gguf.MODEL_ARCH.GRANITE_HYBRID\n    undo_permute = True\n    def __init__(self, *args, **kwargs):\n        # Hybrid mamba models use a prefix for the mamba-specific params.\n        # TODO: Extend this if the prefix(es) need to be configurable\n        self.hparam_prefixes = [\"mamba\"]\n        super().__init__(*args, **kwargs)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "NemotronHModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class NemotronHModel(GraniteHybridModel):\n    \"\"\"Hybrid mamba2/attention model from NVIDIA\"\"\"\n    model_arch = gguf.MODEL_ARCH.NEMOTRON_H\n    is_moe: bool = False\n    def __init__(self, *args, **kwargs):\n        # We have to determine the correct model architecture (MoE vs non-MoE) before\n        # calling the parent __init__. This is because the parent constructor\n        # uses self.model_arch to build the tensor name map, and all MoE-specific\n        # mappings would be missed if it were called with the default non-MoE arch.\n        hparams = ModelBase.load_hparams(args[0], self.is_mistral_format)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LlamaEmbedNemotronModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LlamaEmbedNemotronModel(LlamaModel):\n    model_arch = gguf.MODEL_ARCH.LLAMA_EMBED\n@ModelBase.register(\"BailingMoeForCausalLM\")\nclass BailingMoeModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.BAILINGMOE\n    def set_vocab(self):\n        self._set_vocab_gpt2()\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        hparams = self.hparams",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "BailingMoeModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class BailingMoeModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.BAILINGMOE\n    def set_vocab(self):\n        self._set_vocab_gpt2()\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        hparams = self.hparams\n        if (rope_dim := hparams.get(\"head_dim\")) is None:\n            rope_dim = hparams[\"hidden_size\"] // hparams[\"num_attention_heads\"]\n        self.gguf_writer.add_rope_dimension_count(rope_dim)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "BailingMoeV2Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class BailingMoeV2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.BAILINGMOE2\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if nextn_layers := self.hparams.get(\"num_nextn_predict_layers\", 0):\n            self.block_count = self.hparams[\"num_hidden_layers\"] + nextn_layers\n            self.tensor_map = gguf.get_tensor_name_map(self.model_arch, self.block_count)\n    def set_vocab(self):\n        self._set_vocab_gpt2()\n    def set_gguf_parameters(self):",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "GroveMoeModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class GroveMoeModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.GROVEMOE\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        if (n_experts := self.hparams.get(\"num_experts\")) is not None:\n            self.gguf_writer.add_expert_count(n_experts)\n        if (moe_intermediate_size := self.hparams.get(\"moe_intermediate_size\")) is not None:\n            self.gguf_writer.add_expert_feed_forward_length(moe_intermediate_size)\n            logger.info(f\"gguf: expert feed forward length = {moe_intermediate_size}\")\n        # FIXME?: Hardcoded https://huggingface.co/inclusionAI/GroveMoE-Inst/blob/c4c69e5970d18907b5e6ddccdfd55176fe292df1/modeling_grove_moe.py#L299",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ChameleonModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ChameleonModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.CHAMELEON\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_swin_norm(self.hparams.get(\"swin_norm\", False))\n    def set_vocab(self):\n        self._set_vocab_gpt2()\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        # ignore image tokenizer for now\n        # TODO: remove this once image support is implemented for Chameleon",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "UltravoxModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class UltravoxModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.LLAMA # dummy\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        raise NotImplementedError(\"Ultravox does not have text decoder. Instead, it uses Llama or other models for text. If you want to get the audio encoder, please use --mmproj argument\")\n@ModelBase.register(\"GlmasrModel\")\nclass GlmASRWhisperEncoderModel(MmprojModel):\n    has_vision_encoder = False\n    has_audio_encoder = True\n    def __init__(self, *args, **kwargs):",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "GlmASRWhisperEncoderModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class GlmASRWhisperEncoderModel(MmprojModel):\n    has_vision_encoder = False\n    has_audio_encoder = True\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if \"hidden_size\" not in self.hparams and \"intermediate_size\" not in self.hparams:\n            self.hparams[\"hidden_size\"] = self.hparams[\"d_model\"]\n            self.hparams[\"intermediate_size\"] = self.hparams[\"encoder_ffn_dim\"]\n            self.hparams[\"num_attention_heads\"] = self.hparams[\"encoder_attention_heads\"]\n    def set_gguf_parameters(self):",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "WhisperEncoderModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class WhisperEncoderModel(MmprojModel):\n    has_vision_encoder = False # no vision encoder\n    has_audio_encoder = True\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if \"hidden_size\" not in self.hparams and \"intermediate_size\" not in self.hparams:\n            self.hparams[\"hidden_size\"] = self.hparams[\"d_model\"]\n            self.hparams[\"intermediate_size\"] = self.hparams[\"encoder_ffn_dim\"]\n            self.hparams[\"num_attention_heads\"] = self.hparams[\"encoder_attention_heads\"]\n    def set_gguf_parameters(self):",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "UltravoxWhisperEncoderModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class UltravoxWhisperEncoderModel(WhisperEncoderModel):\n    has_vision_encoder = False # no vision encoder\n    has_audio_encoder = True\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.ULTRAVOX)\n        self.gguf_writer.add_audio_stack_factor(self.global_config[\"stack_factor\"])\n@ModelBase.register(\"VoxtralForConditionalGeneration\")\nclass VoxtralWhisperEncoderModel(WhisperEncoderModel):\n    has_vision_encoder = False # no vision encoder",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "VoxtralWhisperEncoderModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class VoxtralWhisperEncoderModel(WhisperEncoderModel):\n    has_vision_encoder = False # no vision encoder\n    has_audio_encoder = True\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.VOXTRAL)\n        self.gguf_writer.add_audio_stack_factor(4) # == intermediate_size // hidden_size\n@ModelBase.register(\"AudioFlamingo3ForConditionalGeneration\")\nclass AudioFlamingo3WhisperEncoderModel(WhisperEncoderModel):\n    def set_gguf_parameters(self):",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "AudioFlamingo3WhisperEncoderModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class AudioFlamingo3WhisperEncoderModel(WhisperEncoderModel):\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.MUSIC_FLAMINGO)\n    def tensor_force_quant(self, name, new_name, bid, n_dims):\n        if \".conv\" in name and \".weight\" in name:\n            # Was trained in BF16, being safe, avoiding quantizing to FP16\n            return gguf.GGMLQuantizationType.F32\n        return super().tensor_force_quant(name, new_name, bid, n_dims)\n@ModelBase.register(\"FalconH1ForCausalLM\")",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "FalconH1Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class FalconH1Model(Mamba2Model):\n    model_arch = gguf.MODEL_ARCH.FALCON_H1\n    def __init__(self, *args, **kwargs):\n        # Set the hparam prefixes for Falcon Mamba2\n        self.hparam_prefixes = [\"mamba\"]\n        # Initialize the base Mamba2Model\n        super().__init__(*args, **kwargs)\n        # Use Llama conversion for attention\n        self._transformer_model_class = LlamaModel\n        # n_group and d_inner are used during reshape_tensors for mamba2",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "HunYuanMoEModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class HunYuanMoEModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.HUNYUAN_MOE\n    def set_vocab(self):\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(self.dir_model, trust_remote_code=True)\n        # 1. Get the pre-tokenizer identifier hash\n        tokpre = self.get_vocab_base_pre(tokenizer)\n        # 2. Reverse-engineer the merges list from mergeable_ranks\n        merges = []\n        vocab = {}",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LLaDAMoEModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LLaDAMoEModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.LLADA_MOE\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        if (n_experts := self.hparams.get(\"num_experts\")) is not None:\n            self.gguf_writer.add_expert_count(n_experts)\n        if (expert_intermediate_size := self.hparams.get(\"expert_intermediate_size\")) is not None:\n            self.gguf_writer.add_expert_feed_forward_length(expert_intermediate_size)\n        # number of experts used per token (top-k)\n        if (n_experts_used := self.hparams.get(\"num_experts_per_tok\")) is not None:",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "HunYuanModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class HunYuanModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.HUNYUAN_DENSE\n    def set_vocab(self):\n        if (self.dir_model / \"tokenizer.json\").is_file():\n            self._set_vocab_gpt2()\n        else:\n            from transformers import AutoTokenizer\n            tokenizer = AutoTokenizer.from_pretrained(self.dir_model, trust_remote_code=True)\n            # 1. Get the pre-tokenizer identifier hash\n            tokpre = self.get_vocab_base_pre(tokenizer)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "SmolLM3Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class SmolLM3Model(LlamaModel):\n    model_arch = gguf.MODEL_ARCH.SMOLLM3\n@ModelBase.register(\"GptOssForCausalLM\")\nclass GptOssModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.GPT_OSS\n    # TODO: remove once MXFP4 is supported more generally\n    def dequant_model(self):\n        quant_config = self.hparams.get(\"quantization_config\")\n        if quant_config is not None and quant_config.get(\"quant_method\") == \"mxfp4\":\n            return",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "GptOssModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class GptOssModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.GPT_OSS\n    # TODO: remove once MXFP4 is supported more generally\n    def dequant_model(self):\n        quant_config = self.hparams.get(\"quantization_config\")\n        if quant_config is not None and quant_config.get(\"quant_method\") == \"mxfp4\":\n            return\n        return super().dequant_model()\n    def transform_nibble_layout(self, tensor):\n        assert tensor.dtype == torch.uint8",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LFM2Model",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LFM2Model(TextModel):\n    model_arch = gguf.MODEL_ARCH.LFM2\n    def _add_feed_forward_length(self):\n        ff_dim = self.hparams[\"block_ff_dim\"]\n        auto_adjust_ff_dim = self.hparams[\"block_auto_adjust_ff_dim\"]\n        ff_dim = self.hparams[\"block_ff_dim\"]\n        ffn_dim_multiplier = self.hparams[\"block_ffn_dim_multiplier\"]\n        multiple_of = self.hparams[\"block_multiple_of\"]\n        if auto_adjust_ff_dim:\n            ff_dim = int(2 * ff_dim / 3)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LFM2ColBertModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LFM2ColBertModel(LFM2Model):\n    model_arch = gguf.MODEL_ARCH.LFM2\n    dense_tensor_name = \"dense_2\"\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        if not name.startswith(self.dense_tensor_name):\n            name = \"model.\" + name\n        yield from super().modify_tensors(data_torch, name, bid)\n    def generate_extra_tensors(self) -> Iterable[tuple[str, Tensor]]:\n        # dense tensor is stored in a separate safetensors file\n        from safetensors.torch import load_file",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LFM2MoeModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LFM2MoeModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.LFM2MOE\n    def set_gguf_parameters(self):\n        # set num_key_value_heads only for attention layers\n        self.hparams[\"num_key_value_heads\"] = [\n            self.hparams[\"num_key_value_heads\"] if layer_type == \"full_attention\" else 0\n            for layer_type in self.hparams[\"layer_types\"]\n        ]\n        super().set_gguf_parameters()\n        self.gguf_writer.add_expert_count(self.hparams[\"num_experts\"])",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LFM2VLModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LFM2VLModel(MmprojModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert self.hparams_vision is not None\n        # TODO(tarek): for dynamic resolution image_size is not specified, setting here for compatibility\n        self.hparams_vision[\"image_size\"] = 256\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.LFM2)\n        self.gguf_writer.add_vision_attention_layernorm_eps(self.find_vparam([\"layer_norm_eps\"]))",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LFM2AudioModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LFM2AudioModel(ConformerAudioModel):\n    has_vision_encoder = False\n    has_audio_encoder = True\n    model_name = \"Lfm2AudioEncoder\"\n    def get_audio_config(self) -> dict[str, Any] | None:\n        return self.global_config.get(\"encoder\")\n    def set_gguf_parameters(self):\n        assert self.hparams_audio is not None\n        self.hparams_audio[\"hidden_size\"] = self.hparams_audio[\"d_model\"]\n        self.hparams_audio[\"intermediate_size\"] = self.hparams_audio[\"d_model\"]",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "SmallThinkerModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class SmallThinkerModel(TextModel):\n    model_arch = gguf.MODEL_ARCH.SMALLTHINKER\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        if (n_experts := self.hparams.get(\"num_experts\", self.hparams.get(\"moe_num_primary_experts\"))) is not None:\n            self.gguf_writer.add_expert_count(n_experts)\n        if (n_experts_used := self.hparams.get(\"num_experts_per_tok\", self.hparams.get(\"moe_num_active_primary_experts\"))) is not None:\n            self.gguf_writer.add_expert_used_count(n_experts_used)\n        if (moe_intermediate_size := self.hparams.get(\"moe_ffn_hidden_size\")) is not None:\n            self.gguf_writer.add_expert_feed_forward_length(moe_intermediate_size)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ModernBertModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ModernBertModel(BertModel):\n    model_arch = gguf.MODEL_ARCH.MODERN_BERT\n    def set_vocab(self):\n        self.gguf_writer.add_add_bos_token(True)\n        self.gguf_writer.add_add_eos_token(True)\n        self.gguf_writer.add_add_sep_token(True)\n        self._set_vocab_gpt2()\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_sliding_window(self.hparams[\"local_attention\"])",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "ApertusModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class ApertusModel(LlamaModel):\n    model_arch = gguf.MODEL_ARCH.APERTUS\n    undo_permute = False\n    _alpha_n = {}\n    _alpha_p = {}\n    _beta = {}\n    _eps = {}\n    def modify_tensors(self, data_torch, name, bid):\n        # Handle xIELU activation parameters\n        n_layers = self.hparams[\"num_hidden_layers\"]",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "MistralModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class MistralModel(LlamaModel):\n    model_arch = gguf.MODEL_ARCH.MISTRAL3\n    model_name = \"Mistral\"\n    hf_arch = \"\"\n    is_mistral_format = True\n    undo_permute = False\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # for compatibility, we use LLAMA arch for older models\n        # TODO: remove this once everyone migrates to newer version of llama.cpp",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "MistralMoeModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class MistralMoeModel(DeepseekV2Model):\n    model_arch = gguf.MODEL_ARCH.DEEPSEEK2\n    model_name = \"Mistral\"\n    hf_arch = \"\"\n    is_mistral_format = True\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        logger.info(\"Using MistralMoeModel\")\n        # remap hparams from Mistral MoE format to DeepseekV2 format\n        # we do this way to be able to reuse DeepseekV2Model set_gguf_parameters logic",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "PixtralModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class PixtralModel(LlavaVisionModel):\n    model_name = \"Pixtral\"\n    hf_arch = \"\"\n    is_mistral_format = True\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.PIXTRAL)\n        self.gguf_writer.add_vision_attention_layernorm_eps(\n            self.find_hparam([\"norm_eps\"])\n        )",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LightOnOCRVisionModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LightOnOCRVisionModel(LlavaVisionModel):\n    is_mistral_format = False\n    use_break_tok = False\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.LIGHTONOCR)\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None):\n        name = name.replace(\"model.vision_encoder.\", \"vision_tower.\")\n        name = name.replace(\"model.vision_projection.\", \"multi_modal_projector.\")\n        yield from super().modify_tensors(data_torch, name, bid)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "KimiVLModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class KimiVLModel(MmprojModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert self.hparams_vision is not None\n        self.hparams_vision[\"image_size\"] = 64 * 14 # for compatibility\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.KIMIVL)\n        self.gguf_writer.add_vision_use_gelu(True)\n        self.gguf_writer.add_vision_projector_scale_factor(2)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "CogVLMVisionModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class CogVLMVisionModel(MmprojModel):\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_vision_attention_layernorm_eps(self.hparams.get(\"layer_norm_eps\", 1e-6))\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.COGVLM)\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        if not name.startswith(\"model.vision.\"):\n            return\n        yield from super().modify_tensors(data_torch, name, bid)\n@ModelBase.register(\"CogVLMForCausalLM\")",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "CogVLMModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class CogVLMModel(LlamaModel):\n    model_arch = gguf.MODEL_ARCH.COGVLM\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        # block vision tensors\n        if name.startswith(\"model.vision.\"):\n            return\n        yield from ModelBase.modify_tensors(self, data_torch, name, bid)\n@ModelBase.register(\"JanusForConditionalGeneration\")\nclass JanusProModel(LlamaModel):\n    model_arch = gguf.MODEL_ARCH.LLAMA  # reuse Llama arch",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "JanusProModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class JanusProModel(LlamaModel):\n    model_arch = gguf.MODEL_ARCH.LLAMA  # reuse Llama arch\n    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n        # Skip vision, aligner, and generation tensors\n        skip_prefixes = (\n            'model.vision_model.',\n            'model.aligner.',\n            'model.vqmodel.',\n            'model.generation_embeddings.',\n            'model.generation_aligner.',",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "JanusProVisionModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class JanusProVisionModel(MmprojModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert self.hparams_vision is not None\n        if \"intermediate_size\" not in self.hparams_vision:\n            mlp_ratio = self.hparams_vision.get(\"mlp_ratio\")\n            hidden_size = self.hparams_vision.get(\"hidden_size\")\n            if mlp_ratio is not None and hidden_size is not None:\n                self.hparams_vision[\"intermediate_size\"] = int(round(hidden_size * mlp_ratio))\n    def set_gguf_parameters(self):",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "YoutuVLVisionModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class YoutuVLVisionModel(MmprojModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert self.hparams_vision is not None\n        self.hparams_vision[\"image_size\"] = self.hparams_vision.get(\"image_size\", 560)\n    def set_gguf_parameters(self):\n        super().set_gguf_parameters()\n        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.YOUTUVL)\n        self.gguf_writer.add_vision_attention_layernorm_eps(self.hparams.get(\"layer_norm_eps\", 1e-6))\n        # Handle activation function",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "SolarOpenModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class SolarOpenModel(Glm4MoeModel):\n    model_arch = gguf.MODEL_ARCH.GLM4_MOE\n    def set_vocab(self):\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(self.dir_model)\n        special_vocab = gguf.SpecialVocab(self.dir_model, load_merges=True)\n        tokens, toktypes, tokpre = self.get_vocab_base()\n        self.gguf_writer.add_tokenizer_model(\"gpt2\")\n        self.gguf_writer.add_tokenizer_pre(tokpre)\n        self.gguf_writer.add_token_list(tokens)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "LazyTorchTensor",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "class LazyTorchTensor(gguf.LazyBase):\n    _tensor_type = torch.Tensor\n    # to keep the type-checker happy\n    dtype: torch.dtype\n    shape: torch.Size\n    # only used when converting a torch.Tensor to a np.ndarray\n    _dtype_map: dict[torch.dtype, type] = {\n        torch.float16: np.float16,\n        torch.float32: np.float32,\n        torch.uint8: np.uint8,",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "def parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(\n        description=\"Convert a huggingface model to a GGML compatible file\")\n    parser.add_argument(\n        \"--vocab-only\", action=\"store_true\",\n        help=\"extract only the vocab\",\n    )\n    parser.add_argument(\n        \"--outfile\", type=Path,\n        help=\"path to write to; default: based on input. {ftype} will be replaced by the outtype.\",",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "split_str_to_n_bytes",
        "kind": 2,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "def split_str_to_n_bytes(split_str: str) -> int:\n    if split_str.endswith(\"K\"):\n        n = int(split_str[:-1]) * 1000\n    elif split_str.endswith(\"M\"):\n        n = int(split_str[:-1]) * 1000 * 1000\n    elif split_str.endswith(\"G\"):\n        n = int(split_str[:-1]) * 1000 * 1000 * 1000\n    elif split_str.isnumeric():\n        n = int(split_str)\n    else:",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "get_model_architecture",
        "kind": 2,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "def get_model_architecture(hparams: dict[str, Any], model_type: ModelType) -> str:\n    # TODO @ngxson : this won't work correctly if the model has both audio & vision encoders\n    # maybe we should fallback to text model's arch in that case, since not many models have both\n    text_config = hparams.get(\"text_config\", {})\n    vision_config = hparams.get(\"vision_config\", {})\n    arch = None\n    if (arches := hparams.get(\"architectures\")) is not None and len(arches) > 0:\n        arch = arches[0]\n    elif \"ssm_cfg\" in hparams:\n        # For non-hf Mamba and Mamba2 models",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "def main() -> None:\n    args = parse_args()\n    if args.print_supported_models:\n        logger.error(\"Supported models:\")\n        ModelBase.print_registered_models()\n        sys.exit(0)\n    if args.verbose:\n        logging.basicConfig(level=logging.DEBUG)\n    else:\n        logging.basicConfig(level=logging.INFO)",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "logger = logging.getLogger(\"hf-to-gguf\")\n###### MODEL DEFINITIONS ######\nclass SentencePieceTokenTypes(IntEnum):\n    NORMAL = 1\n    UNKNOWN = 2\n    CONTROL = 3\n    USER_DEFINED = 4\n    UNUSED = 5\n    BYTE = 6\nclass ModelType(IntEnum):",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "AnyModel",
        "kind": 5,
        "importPath": "llama.cpp.convert_hf_to_gguf",
        "description": "llama.cpp.convert_hf_to_gguf",
        "peekOfCode": "AnyModel = TypeVar(\"AnyModel\", bound=\"type[ModelBase]\")\nclass ModelBase:\n    _model_classes: dict[ModelType, dict[str, type[ModelBase]]] = {\n        ModelType.TEXT: {},\n        ModelType.MMPROJ: {},\n    }\n    dir_model: Path\n    ftype: gguf.LlamaFileType\n    fname_out: Path\n    is_big_endian: bool",
        "detail": "llama.cpp.convert_hf_to_gguf",
        "documentation": {}
    },
    {
        "label": "TOKENIZER_TYPE",
        "kind": 6,
        "importPath": "llama.cpp.convert_hf_to_gguf_update",
        "description": "llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "class TOKENIZER_TYPE(IntEnum):\n    SPM = auto()\n    BPE = auto()\n    WPM = auto()\n    UGM = auto()\nDOC_STRING = \"\"\"\nThis script downloads the tokenizer models of the specified models from Huggingface and\ngenerates the get_vocab_base_pre() function for convert_hf_to_gguf.py\n/!\\\\ It is intended to be used by contributors and is not meant to be run by end users\nThis is necessary in order to analyze the type of pre-tokenizer used by the model and",
        "detail": "llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "download_file_with_auth",
        "kind": 2,
        "importPath": "llama.cpp.convert_hf_to_gguf_update",
        "description": "llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "def download_file_with_auth(url, token, save_path):\n    headers = {\"Authorization\": f\"Bearer {token}\"} if token else None\n    response = sess.get(url, headers=headers)\n    response.raise_for_status()\n    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n    with open(save_path, 'wb') as downloaded_file:\n        downloaded_file.write(response.content)\n    logger.info(f\"File {save_path} downloaded successfully\")\ndef download_model(model):\n    name = model[\"name\"]",
        "detail": "llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "download_model",
        "kind": 2,
        "importPath": "llama.cpp.convert_hf_to_gguf_update",
        "description": "llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "def download_model(model):\n    name = model[\"name\"]\n    repo = model[\"repo\"]\n    tokt = model[\"tokt\"]\n    os.makedirs(f\"models/tokenizers/{name}\", exist_ok=True)\n    files = [\"config.json\", \"tokenizer.json\", \"tokenizer_config.json\"]\n    if name == \"gpt-4o\":\n        # Xenova/gpt-4o is tokenizer-only, it does not contain config.json\n        files = [\"tokenizer.json\", \"tokenizer_config.json\"]\n    if tokt == TOKENIZER_TYPE.SPM:",
        "detail": "llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "get_existing_models",
        "kind": 2,
        "importPath": "llama.cpp.convert_hf_to_gguf_update",
        "description": "llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "def get_existing_models(convert_py):\n    pattern = r'if chkhsh == \"([a-f0-9]{64})\":\\s*\\n\\s*.*\\s*res = \"([^\"]+)\"'\n    matches = re.findall(pattern, convert_py)\n    output = {}\n    for chkhsh, res in matches:\n        output[res] = chkhsh\n    return output\nexisting_models = {}\nall_models = models.copy()\nif not args.full:",
        "detail": "llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "llama.cpp.convert_hf_to_gguf_update",
        "description": "llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "logger = logging.getLogger(\"convert_hf_to_gguf_update\")\nsess = requests.Session()\nconvert_py_pth = pathlib.Path(\"convert_hf_to_gguf.py\")\nconvert_py = convert_py_pth.read_text(encoding=\"utf-8\")\nhf_token_pth = pathlib.Path.home() / \".cache\" / \"huggingface\" / \"token\"\nhf_token = hf_token_pth.read_text(encoding=\"utf-8\").strip() if hf_token_pth.exists() else None\nclass TOKENIZER_TYPE(IntEnum):\n    SPM = auto()\n    BPE = auto()\n    WPM = auto()",
        "detail": "llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "sess",
        "kind": 5,
        "importPath": "llama.cpp.convert_hf_to_gguf_update",
        "description": "llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "sess = requests.Session()\nconvert_py_pth = pathlib.Path(\"convert_hf_to_gguf.py\")\nconvert_py = convert_py_pth.read_text(encoding=\"utf-8\")\nhf_token_pth = pathlib.Path.home() / \".cache\" / \"huggingface\" / \"token\"\nhf_token = hf_token_pth.read_text(encoding=\"utf-8\").strip() if hf_token_pth.exists() else None\nclass TOKENIZER_TYPE(IntEnum):\n    SPM = auto()\n    BPE = auto()\n    WPM = auto()\n    UGM = auto()",
        "detail": "llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "convert_py_pth",
        "kind": 5,
        "importPath": "llama.cpp.convert_hf_to_gguf_update",
        "description": "llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "convert_py_pth = pathlib.Path(\"convert_hf_to_gguf.py\")\nconvert_py = convert_py_pth.read_text(encoding=\"utf-8\")\nhf_token_pth = pathlib.Path.home() / \".cache\" / \"huggingface\" / \"token\"\nhf_token = hf_token_pth.read_text(encoding=\"utf-8\").strip() if hf_token_pth.exists() else None\nclass TOKENIZER_TYPE(IntEnum):\n    SPM = auto()\n    BPE = auto()\n    WPM = auto()\n    UGM = auto()\nDOC_STRING = \"\"\"",
        "detail": "llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "convert_py",
        "kind": 5,
        "importPath": "llama.cpp.convert_hf_to_gguf_update",
        "description": "llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "convert_py = convert_py_pth.read_text(encoding=\"utf-8\")\nhf_token_pth = pathlib.Path.home() / \".cache\" / \"huggingface\" / \"token\"\nhf_token = hf_token_pth.read_text(encoding=\"utf-8\").strip() if hf_token_pth.exists() else None\nclass TOKENIZER_TYPE(IntEnum):\n    SPM = auto()\n    BPE = auto()\n    WPM = auto()\n    UGM = auto()\nDOC_STRING = \"\"\"\nThis script downloads the tokenizer models of the specified models from Huggingface and",
        "detail": "llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "hf_token_pth",
        "kind": 5,
        "importPath": "llama.cpp.convert_hf_to_gguf_update",
        "description": "llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "hf_token_pth = pathlib.Path.home() / \".cache\" / \"huggingface\" / \"token\"\nhf_token = hf_token_pth.read_text(encoding=\"utf-8\").strip() if hf_token_pth.exists() else None\nclass TOKENIZER_TYPE(IntEnum):\n    SPM = auto()\n    BPE = auto()\n    WPM = auto()\n    UGM = auto()\nDOC_STRING = \"\"\"\nThis script downloads the tokenizer models of the specified models from Huggingface and\ngenerates the get_vocab_base_pre() function for convert_hf_to_gguf.py",
        "detail": "llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "hf_token",
        "kind": 5,
        "importPath": "llama.cpp.convert_hf_to_gguf_update",
        "description": "llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "hf_token = hf_token_pth.read_text(encoding=\"utf-8\").strip() if hf_token_pth.exists() else None\nclass TOKENIZER_TYPE(IntEnum):\n    SPM = auto()\n    BPE = auto()\n    WPM = auto()\n    UGM = auto()\nDOC_STRING = \"\"\"\nThis script downloads the tokenizer models of the specified models from Huggingface and\ngenerates the get_vocab_base_pre() function for convert_hf_to_gguf.py\n/!\\\\ It is intended to be used by contributors and is not meant to be run by end users",
        "detail": "llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "DOC_STRING",
        "kind": 5,
        "importPath": "llama.cpp.convert_hf_to_gguf_update",
        "description": "llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "DOC_STRING = \"\"\"\nThis script downloads the tokenizer models of the specified models from Huggingface and\ngenerates the get_vocab_base_pre() function for convert_hf_to_gguf.py\n/!\\\\ It is intended to be used by contributors and is not meant to be run by end users\nThis is necessary in order to analyze the type of pre-tokenizer used by the model and\nprovide the necessary information to llama.cpp via the GGUF header in order to implement\nthe same pre-tokenizer.\nref: https://github.com/ggml-org/llama.cpp/pull/6920\nInstructions:\n- Add a new model to the \"models\" list",
        "detail": "llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "llama.cpp.convert_hf_to_gguf_update",
        "description": "llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "parser = argparse.ArgumentParser(description=DOC_STRING, formatter_class=argparse.RawTextHelpFormatter)\nparser.add_argument(\n    \"--full\", action=\"store_true\",\n    help=\"download full list of models - make sure you have access to all of them\",\n)\nparser.add_argument(\n    \"--check-missing\", action=\"store_true\",\n    help=\"only check for missing pre-tokenizer hashes\",\n)\nparser.add_argument(",
        "detail": "llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "llama.cpp.convert_hf_to_gguf_update",
        "description": "llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "args = parser.parse_args()\nhf_token = args.hf_token if args.hf_token is not None else hf_token\nif hf_token is None:\n    logger.warning(\"HF token not found. You can provide it as an argument or set it in ~/.cache/huggingface/token\")\nif args.check_missing and args.full:\n    logger.warning(\"Downloading full list of models requested, ignoring --check-missing!\")\n    args.check_missing = False\n# TODO: this string has to exercise as much pre-tokenizer functionality as possible\n#       will be updated with time - contributions welcome\nCHK_TXT = '\\n \\n\\n \\n\\n\\n \\t \\t\\t \\t\\n  \\n   \\n    \\n     \\n (normal)  (multiple emojis concatenated)   3 33 333 3333 33333 333333 3333333 33333333 3.3 3..3 3...3  ?apple1314151 ------=======    \\'\\'\\'\\'\\'\\'```````\\\"\\\"\\\"\\\"......!!!!!!?????? I\\'ve been \\'told he\\'s there, \\'RE you sure? \\'M not sure I\\'ll make it, \\'D you like some tea? We\\'Ve a\\'lL'",
        "detail": "llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "hf_token",
        "kind": 5,
        "importPath": "llama.cpp.convert_hf_to_gguf_update",
        "description": "llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "hf_token = args.hf_token if args.hf_token is not None else hf_token\nif hf_token is None:\n    logger.warning(\"HF token not found. You can provide it as an argument or set it in ~/.cache/huggingface/token\")\nif args.check_missing and args.full:\n    logger.warning(\"Downloading full list of models requested, ignoring --check-missing!\")\n    args.check_missing = False\n# TODO: this string has to exercise as much pre-tokenizer functionality as possible\n#       will be updated with time - contributions welcome\nCHK_TXT = '\\n \\n\\n \\n\\n\\n \\t \\t\\t \\t\\n  \\n   \\n    \\n     \\n (normal)  (multiple emojis concatenated)   3 33 333 3333 33333 333333 3333333 33333333 3.3 3..3 3...3  ?apple1314151 ------=======    \\'\\'\\'\\'\\'\\'```````\\\"\\\"\\\"\\\"......!!!!!!?????? I\\'ve been \\'told he\\'s there, \\'RE you sure? \\'M not sure I\\'ll make it, \\'D you like some tea? We\\'Ve a\\'lL'\n# TODO: add models here, base models preferred",
        "detail": "llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "CHK_TXT",
        "kind": 5,
        "importPath": "llama.cpp.convert_hf_to_gguf_update",
        "description": "llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "CHK_TXT = '\\n \\n\\n \\n\\n\\n \\t \\t\\t \\t\\n  \\n   \\n    \\n     \\n (normal)  (multiple emojis concatenated)   3 33 333 3333 33333 333333 3333333 33333333 3.3 3..3 3...3  ?apple1314151 ------=======    \\'\\'\\'\\'\\'\\'```````\\\"\\\"\\\"\\\"......!!!!!!?????? I\\'ve been \\'told he\\'s there, \\'RE you sure? \\'M not sure I\\'ll make it, \\'D you like some tea? We\\'Ve a\\'lL'\n# TODO: add models here, base models preferred\nmodels = [\n    {\"name\": \"llama-spm\",        \"tokt\": TOKENIZER_TYPE.SPM, \"repo\": \"https://huggingface.co/meta-llama/Llama-2-7b-hf\", },\n    {\"name\": \"llama-bpe\",        \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/meta-llama/Meta-Llama-3-8B\", },\n    {\"name\": \"phi-3\",            \"tokt\": TOKENIZER_TYPE.SPM, \"repo\": \"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\", },\n    {\"name\": \"deepseek-llm\",     \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/deepseek-ai/deepseek-llm-7b-base\", },\n    {\"name\": \"deepseek-coder\",   \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-base\", },\n    {\"name\": \"falcon\",           \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/tiiuae/falcon-7b\", },\n    {\"name\": \"bert-bge\",         \"tokt\": TOKENIZER_TYPE.WPM, \"repo\": \"https://huggingface.co/BAAI/bge-small-en-v1.5\", },",
        "detail": "llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "models",
        "kind": 5,
        "importPath": "llama.cpp.convert_hf_to_gguf_update",
        "description": "llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "models = [\n    {\"name\": \"llama-spm\",        \"tokt\": TOKENIZER_TYPE.SPM, \"repo\": \"https://huggingface.co/meta-llama/Llama-2-7b-hf\", },\n    {\"name\": \"llama-bpe\",        \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/meta-llama/Meta-Llama-3-8B\", },\n    {\"name\": \"phi-3\",            \"tokt\": TOKENIZER_TYPE.SPM, \"repo\": \"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\", },\n    {\"name\": \"deepseek-llm\",     \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/deepseek-ai/deepseek-llm-7b-base\", },\n    {\"name\": \"deepseek-coder\",   \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-base\", },\n    {\"name\": \"falcon\",           \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/tiiuae/falcon-7b\", },\n    {\"name\": \"bert-bge\",         \"tokt\": TOKENIZER_TYPE.WPM, \"repo\": \"https://huggingface.co/BAAI/bge-small-en-v1.5\", },\n    {\"name\": \"falcon3\",          \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/tiiuae/Falcon3-7B-Base\", },\n    {\"name\": \"bert-bge-large\",   \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/BAAI/bge-large-zh-v1.5\", },",
        "detail": "llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "pre_computed_hashes",
        "kind": 5,
        "importPath": "llama.cpp.convert_hf_to_gguf_update",
        "description": "llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "pre_computed_hashes = [\n    # chatglm-bpe has 2 hashes, why?\n    {\"name\": \"chatglm-bpe\", \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/THUDM/glm-4-9b-chat\", \"chkhsh\": \"b6e8e1518dc4305be2fe39c313ed643381c4da5db34a98f6a04c093f8afbe99b\"},\n    {\"name\": \"chatglm-bpe\", \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/THUDM/glm-4-9b-chat\", \"chkhsh\": \"81d72c7348a9f0ebe86f23298d37debe0a5e71149e29bd283904c02262b27516\"},\n    {\"name\": \"glm4\", \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/THUDM/glm-4-9b-hf\", \"chkhsh\": \"a1336059768a55c99a734006ffb02203cd450fed003e9a71886c88acf24fdbc2\"},\n    {\"name\": \"glm4\", \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/zai-org/GLM-4.5-Air\", \"chkhsh\": \"9ca2dd618e8afaf09731a7cf6e2105b373ba6a1821559f258b272fe83e6eb902\"},\n    {\"name\": \"minerva-7b\", \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/sapienzanlp/Minerva-7B-base-v1.0\", \"chkhsh\": \"1431a23e583c97432bc230bff598d103ddb5a1f89960c8f1d1051aaa944d0b35\"},\n    {\"name\": \"hunyuan\", \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/tencent/Hunyuan-A13B-Instruct\", \"chkhsh\": \"7e57df22b1fe23a7b1e1c7f3dc4e3f96d43a4eb0836d0c6bdc3436d7b2f1c664\"},\n    {\"name\": \"hunyuan-dense\", \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/tencent/Hunyuan-4B-Instruct\", \"chkhsh\": \"bba3b3366b646dbdded5dbc42d59598b849371afc42f7beafa914afaa5b70aa6\"},\n    # falcon-h1 series uses 4 different tokenizers across model sizes (0.5b - 34b), hence we need to define 4 different hashes",
        "detail": "llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "existing_models",
        "kind": 5,
        "importPath": "llama.cpp.convert_hf_to_gguf_update",
        "description": "llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "existing_models = {}\nall_models = models.copy()\nif not args.full:\n    # Filter out models that already exist in convert_hf_to_gguf.py\n    existing_models = get_existing_models(convert_py)\n    all_models = models.copy()\n    models = [model for model in all_models if model[\"name\"] not in existing_models]\nif not args.check_missing:\n    logging.info(f\"Downloading {len(models)} models...\")\n    for model in models:",
        "detail": "llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "all_models",
        "kind": 5,
        "importPath": "llama.cpp.convert_hf_to_gguf_update",
        "description": "llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "all_models = models.copy()\nif not args.full:\n    # Filter out models that already exist in convert_hf_to_gguf.py\n    existing_models = get_existing_models(convert_py)\n    all_models = models.copy()\n    models = [model for model in all_models if model[\"name\"] not in existing_models]\nif not args.check_missing:\n    logging.info(f\"Downloading {len(models)} models...\")\n    for model in models:\n        try:",
        "detail": "llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "src_ifs",
        "kind": 5,
        "importPath": "llama.cpp.convert_hf_to_gguf_update",
        "description": "llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "src_ifs = \"\"\nfor model in [*pre_computed_hashes, *all_models]:\n    name = model[\"name\"]\n    tokt = model[\"tokt\"]\n    chkhsh = model.get(\"chkhsh\")\n    if tokt == TOKENIZER_TYPE.SPM or tokt == TOKENIZER_TYPE.UGM:\n        continue\n    # create the tokenizer\n    if chkhsh is not None:\n        # if the model has a pre-computed hash, use it",
        "detail": "llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "src_func",
        "kind": 5,
        "importPath": "llama.cpp.convert_hf_to_gguf_update",
        "description": "llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "src_func = f\"\"\"\n    def get_vocab_base_pre(self, tokenizer) -> str:\n        # encoding this string and hashing the resulting tokens would (hopefully) give us a unique identifier that\n        # is specific for the BPE pre-tokenizer used by the model\n        # we will use this unique identifier to write a \"tokenizer.ggml.pre\" entry in the GGUF file which we can\n        # use in llama.cpp to implement the same pre-tokenizer\n        chktxt = {repr(CHK_TXT)}\n        chktok = tokenizer.encode(chktxt)\n        chkhsh = sha256(str(chktok).encode()).hexdigest()\n        logger.debug(f\"chktok: {{chktok}}\")",
        "detail": "llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "convert_py",
        "kind": 5,
        "importPath": "llama.cpp.convert_hf_to_gguf_update",
        "description": "llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "convert_py = re.sub(\n    r\"(# Marker: Start get_vocab_base_pre)(.+?)( +# Marker: End get_vocab_base_pre)\",\n    lambda m: m.group(1) + src_func + m.group(3),\n    convert_py,\n    flags=re.DOTALL | re.MULTILINE,\n)\nconvert_py_pth.write_text(convert_py, encoding=\"utf-8\")\nlogger.info(\"+++ convert_hf_to_gguf.py was updated\")\n# generate tests for each tokenizer model\ntests = [",
        "detail": "llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "tests",
        "kind": 5,
        "importPath": "llama.cpp.convert_hf_to_gguf_update",
        "description": "llama.cpp.convert_hf_to_gguf_update",
        "peekOfCode": "tests = [\n    \"ied 4  months\",\n    \"pfel\",\n    \"\",\n    \" \",\n    \"  \",\n    \"   \",\n    \"\\t\",\n    \"\\n\",\n    \"\\n\\n\",",
        "detail": "llama.cpp.convert_hf_to_gguf_update",
        "documentation": {}
    },
    {
        "label": "GGMLFormat",
        "kind": 6,
        "importPath": "llama.cpp.convert_llama_ggml_to_gguf",
        "description": "llama.cpp.convert_llama_ggml_to_gguf",
        "peekOfCode": "class GGMLFormat(IntEnum):\n    GGML = 0\n    GGMF = 1\n    GGJT = 2\nclass GGMLFType(IntEnum):\n    ALL_F32              = 0\n    MOSTLY_F16           = 1\n    MOSTLY_Q4_0          = 2\n    MOSTLY_Q4_1          = 3\n    MOSTLY_Q4_1_SOME_F16 = 4",
        "detail": "llama.cpp.convert_llama_ggml_to_gguf",
        "documentation": {}
    },
    {
        "label": "GGMLFType",
        "kind": 6,
        "importPath": "llama.cpp.convert_llama_ggml_to_gguf",
        "description": "llama.cpp.convert_llama_ggml_to_gguf",
        "peekOfCode": "class GGMLFType(IntEnum):\n    ALL_F32              = 0\n    MOSTLY_F16           = 1\n    MOSTLY_Q4_0          = 2\n    MOSTLY_Q4_1          = 3\n    MOSTLY_Q4_1_SOME_F16 = 4\n    MOSTLY_Q8_0          = 7\n    MOSTLY_Q5_0          = 8\n    MOSTLY_Q5_1          = 9\n    MOSTLY_Q2_K          = 10",
        "detail": "llama.cpp.convert_llama_ggml_to_gguf",
        "documentation": {}
    },
    {
        "label": "Hyperparameters",
        "kind": 6,
        "importPath": "llama.cpp.convert_llama_ggml_to_gguf",
        "description": "llama.cpp.convert_llama_ggml_to_gguf",
        "peekOfCode": "class Hyperparameters:\n    def __init__(self):\n        self.n_vocab = self.n_embd = self.n_mult = self.n_head = 0\n        self.n_layer = self.n_rot = self.n_ff = 0\n        self.ftype = GGMLFType.ALL_F32\n    def set_n_ff(self, model):\n        ff_tensor_idx = model.tensor_map.get(b'layers.0.feed_forward.w1.weight')\n        assert ff_tensor_idx is not None, 'Missing layer 0 FF tensor'\n        ff_tensor = model.tensors[ff_tensor_idx]\n        self.n_ff = ff_tensor.dims[1]",
        "detail": "llama.cpp.convert_llama_ggml_to_gguf",
        "documentation": {}
    },
    {
        "label": "Vocab",
        "kind": 6,
        "importPath": "llama.cpp.convert_llama_ggml_to_gguf",
        "description": "llama.cpp.convert_llama_ggml_to_gguf",
        "peekOfCode": "class Vocab:\n    def __init__(self, load_scores = True):\n        self.items = []\n        self.load_scores = load_scores\n    def load(self, data, offset, n_vocab):\n        orig_offset = offset\n        for _ in range(n_vocab):\n            itemlen = struct.unpack('<I', data[offset:offset + 4])[0]\n            assert itemlen < 4096, 'Absurd vocab item length'\n            offset += 4",
        "detail": "llama.cpp.convert_llama_ggml_to_gguf",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "kind": 6,
        "importPath": "llama.cpp.convert_llama_ggml_to_gguf",
        "description": "llama.cpp.convert_llama_ggml_to_gguf",
        "peekOfCode": "class Tensor:\n    def __init__(self, use_padding = True):\n        self.name = None\n        self.dims: tuple[int, ...] = ()\n        self.dtype = None\n        self.start_offset = 0\n        self.len_bytes = np.int64(0)\n        self.use_padding = use_padding\n    def load(self, data, offset):\n        orig_offset = offset",
        "detail": "llama.cpp.convert_llama_ggml_to_gguf",
        "documentation": {}
    },
    {
        "label": "GGMLModel",
        "kind": 6,
        "importPath": "llama.cpp.convert_llama_ggml_to_gguf",
        "description": "llama.cpp.convert_llama_ggml_to_gguf",
        "peekOfCode": "class GGMLModel:\n    file_format: GGMLFormat\n    format_version: int\n    def __init__(self):\n        self.hyperparameters = None\n        self.vocab = None\n        self.tensor_map = {}\n        self.tensors = []\n    def validate_header(self, data, offset):\n        magic = bytes(data[offset:offset + 4])",
        "detail": "llama.cpp.convert_llama_ggml_to_gguf",
        "documentation": {}
    },
    {
        "label": "GGMLToGGUF",
        "kind": 6,
        "importPath": "llama.cpp.convert_llama_ggml_to_gguf",
        "description": "llama.cpp.convert_llama_ggml_to_gguf",
        "peekOfCode": "class GGMLToGGUF:\n    def __init__(self, ggml_model, data, cfg, params_override = None, vocab_override = None, special_vocab = None):\n        hp = ggml_model.hyperparameters\n        self.model = ggml_model\n        self.data = data\n        self.cfg = cfg\n        self.params_override = params_override\n        self.vocab_override = vocab_override\n        self.special_vocab = special_vocab\n        if params_override is not None:",
        "detail": "llama.cpp.convert_llama_ggml_to_gguf",
        "documentation": {}
    },
    {
        "label": "handle_metadata",
        "kind": 2,
        "importPath": "llama.cpp.convert_llama_ggml_to_gguf",
        "description": "llama.cpp.convert_llama_ggml_to_gguf",
        "peekOfCode": "def handle_metadata(cfg, hp):\n    import examples.convert_legacy_llama as convert\n    assert cfg.model_metadata_dir.is_dir(), 'Metadata dir is not a directory'\n    hf_config_path   = cfg.model_metadata_dir / \"config.json\"\n    orig_config_path = cfg.model_metadata_dir / \"params.json\"\n    # We pass a fake model here. \"original\" mode will check the shapes of some\n    # tensors if information is missing in the .json file: other than that, the\n    # model data isn't used so this should be safe (at least for now).\n    fakemodel = {\n        'tok_embeddings.weight': convert.LazyTensor.__new__(convert.LazyTensor),",
        "detail": "llama.cpp.convert_llama_ggml_to_gguf",
        "documentation": {}
    },
    {
        "label": "handle_args",
        "kind": 2,
        "importPath": "llama.cpp.convert_llama_ggml_to_gguf",
        "description": "llama.cpp.convert_llama_ggml_to_gguf",
        "peekOfCode": "def handle_args():\n    parser = argparse.ArgumentParser(description = 'Convert GGML models to GGUF')\n    parser.add_argument('--input', '-i', type = Path, required = True,\n                        help = 'Input GGMLv3 filename')\n    parser.add_argument('--output', '-o', type = Path, required = True,\n                        help ='Output GGUF filename')\n    parser.add_argument('--name',\n                        help = 'Set model name')\n    parser.add_argument('--desc',\n                        help = 'Set model description')",
        "detail": "llama.cpp.convert_llama_ggml_to_gguf",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "llama.cpp.convert_llama_ggml_to_gguf",
        "description": "llama.cpp.convert_llama_ggml_to_gguf",
        "peekOfCode": "def main():\n    cfg = handle_args()\n    logging.basicConfig(level=logging.DEBUG if cfg.verbose else logging.INFO)\n    logger.info(f'* Using config: {cfg}')\n    logger.warning('=== WARNING === Be aware that this conversion script is best-effort. Use a native GGUF model if possible. === WARNING ===')\n    if cfg.model_metadata_dir is None and (cfg.gqa == 1 or cfg.eps == '5.0e-06'):\n        logger.info('- Note: If converting LLaMA2, specifying \"--eps 1e-5\" is required. 70B models also need \"--gqa 8\".')\n    data = np.memmap(cfg.input, mode = 'r')\n    model = GGMLModel()\n    logger.info('* Scanning GGML input file')",
        "detail": "llama.cpp.convert_llama_ggml_to_gguf",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "llama.cpp.convert_llama_ggml_to_gguf",
        "description": "llama.cpp.convert_llama_ggml_to_gguf",
        "peekOfCode": "logger = logging.getLogger(\"ggml-to-gguf\")\nclass GGMLFormat(IntEnum):\n    GGML = 0\n    GGMF = 1\n    GGJT = 2\nclass GGMLFType(IntEnum):\n    ALL_F32              = 0\n    MOSTLY_F16           = 1\n    MOSTLY_Q4_0          = 2\n    MOSTLY_Q4_1          = 3",
        "detail": "llama.cpp.convert_llama_ggml_to_gguf",
        "documentation": {}
    },
    {
        "label": "PartialLoraTensor",
        "kind": 6,
        "importPath": "llama.cpp.convert_lora_to_gguf",
        "description": "llama.cpp.convert_lora_to_gguf",
        "peekOfCode": "class PartialLoraTensor:\n    A: Tensor | None = None\n    B: Tensor | None = None\n# magic to support tensor shape modifications and splitting\nclass LoraTorchTensor:\n    _lora_A: Tensor  # (n_rank, row_size)\n    _lora_B: Tensor  # (col_size, n_rank)\n    _rank: int\n    def __init__(self, A: Tensor, B: Tensor):\n        assert len(A.shape) == len(B.shape)",
        "detail": "llama.cpp.convert_lora_to_gguf",
        "documentation": {}
    },
    {
        "label": "LoraTorchTensor",
        "kind": 6,
        "importPath": "llama.cpp.convert_lora_to_gguf",
        "description": "llama.cpp.convert_lora_to_gguf",
        "peekOfCode": "class LoraTorchTensor:\n    _lora_A: Tensor  # (n_rank, row_size)\n    _lora_B: Tensor  # (col_size, n_rank)\n    _rank: int\n    def __init__(self, A: Tensor, B: Tensor):\n        assert len(A.shape) == len(B.shape)\n        assert A.shape[-2] == B.shape[-1]\n        if A.dtype != B.dtype:\n            A = A.to(torch.float32)\n            B = B.to(torch.float32)",
        "detail": "llama.cpp.convert_lora_to_gguf",
        "documentation": {}
    },
    {
        "label": "get_base_tensor_name",
        "kind": 2,
        "importPath": "llama.cpp.convert_lora_to_gguf",
        "description": "llama.cpp.convert_lora_to_gguf",
        "peekOfCode": "def get_base_tensor_name(lora_tensor_name: str) -> str:\n    base_name = lora_tensor_name.replace(\"base_model.model.\", \"\")\n    base_name = base_name.replace(\".lora_A.weight\", \".weight\")\n    base_name = base_name.replace(\".lora_B.weight\", \".weight\")\n    # models produced by mergekit-extract-lora have token embeddings in the adapter\n    base_name = base_name.replace(\".lora_embedding_A\", \".weight\")\n    base_name = base_name.replace(\".lora_embedding_B\", \".weight\")\n    return base_name\ndef parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(",
        "detail": "llama.cpp.convert_lora_to_gguf",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "llama.cpp.convert_lora_to_gguf",
        "description": "llama.cpp.convert_lora_to_gguf",
        "peekOfCode": "def parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(\n        description=\"Convert a Hugging Face PEFT LoRA adapter to a GGUF file\")\n    parser.add_argument(\n        \"--outfile\", type=Path,\n        help=\"path to write to; default: based on input. {ftype} will be replaced by the outtype.\",\n    )\n    parser.add_argument(\n        \"--outtype\", type=str, choices=[\"f32\", \"f16\", \"bf16\", \"q8_0\", \"auto\"], default=\"f32\",\n        help=\"output format - use f32 for float32, f16 for float16, bf16 for bfloat16, q8_0 for Q8_0, auto for the highest-fidelity 16-bit float type depending on the first loaded tensor type\",",
        "detail": "llama.cpp.convert_lora_to_gguf",
        "documentation": {}
    },
    {
        "label": "load_hparams_from_hf",
        "kind": 2,
        "importPath": "llama.cpp.convert_lora_to_gguf",
        "description": "llama.cpp.convert_lora_to_gguf",
        "peekOfCode": "def load_hparams_from_hf(hf_model_id: str) -> tuple[dict[str, Any], Path | None]:\n    from huggingface_hub import try_to_load_from_cache\n    # normally, adapter does not come with base model config, we need to load it from AutoConfig\n    config = AutoConfig.from_pretrained(hf_model_id)\n    cache_dir = try_to_load_from_cache(hf_model_id, \"config.json\")\n    cache_dir = Path(cache_dir).parent if isinstance(cache_dir, str) else None\n    return config.to_dict(), cache_dir\nif __name__ == '__main__':\n    args = parse_args()\n    logging.basicConfig(level=logging.DEBUG if args.verbose else logging.INFO)",
        "detail": "llama.cpp.convert_lora_to_gguf",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "llama.cpp.convert_lora_to_gguf",
        "description": "llama.cpp.convert_lora_to_gguf",
        "peekOfCode": "logger = logging.getLogger(\"lora-to-gguf\")\n@dataclass\nclass PartialLoraTensor:\n    A: Tensor | None = None\n    B: Tensor | None = None\n# magic to support tensor shape modifications and splitting\nclass LoraTorchTensor:\n    _lora_A: Tensor  # (n_rank, row_size)\n    _lora_B: Tensor  # (col_size, n_rank)\n    _rank: int",
        "detail": "llama.cpp.convert_lora_to_gguf",
        "documentation": {}
    },
    {
        "label": "WAKE_WORDS",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "WAKE_WORDS = [\n    \"ira\", \"vira\", \"\",     # Ira (Vira)\n    \"aiva\", \"ava\", \"\",      # Aiva\n    \"ziva\", \"\",           # Ziva\n    \"kiva\", \"\",            # Kiva\n    \"tiva\", \"\",            # Tiva\n    \"reva\", \"\",            # Reva\n    \"niva\", \"\"             # Niva\n]\n# Audio Noise Threshold",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "NOISE_THRESHOLD",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "NOISE_THRESHOLD = 800\n# Messages\nMSG_CAMERA_NOT_FOUND = \"   \"  # No camera found\nMSG_CAMERA_OPENING = \"   \"      # Opening camera\nMSG_PHOTO_MODE = \"       \"\nMSG_VIDEO_MODE = \"       \"",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "MSG_CAMERA_NOT_FOUND",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "MSG_CAMERA_NOT_FOUND = \"   \"  # No camera found\nMSG_CAMERA_OPENING = \"   \"      # Opening camera\nMSG_PHOTO_MODE = \"       \"\nMSG_VIDEO_MODE = \"       \"",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "MSG_CAMERA_OPENING",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "MSG_CAMERA_OPENING = \"   \"      # Opening camera\nMSG_PHOTO_MODE = \"       \"\nMSG_VIDEO_MODE = \"       \"",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "MSG_PHOTO_MODE",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "MSG_PHOTO_MODE = \"       \"\nMSG_VIDEO_MODE = \"       \"",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "MSG_VIDEO_MODE",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "MSG_VIDEO_MODE = \"       \"",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "history_kb",
        "kind": 5,
        "importPath": "create_knowledge_base",
        "description": "create_knowledge_base",
        "peekOfCode": "history_kb = {\n    \"\": \"\",\n    \"\": \"       \",\n    \" \": \"      5     12      \",\n    \" \": \"     12   18     \",\n    \" \": \"     18     \",\n    \" \": \"       ,      \",\n    \" \": \"      ,         \",\n    \"\": \"    ,          \",\n    \"\": \"       \",",
        "detail": "create_knowledge_base",
        "documentation": {}
    },
    {
        "label": "indian_history_kb",
        "kind": 5,
        "importPath": "create_knowledge_base",
        "description": "create_knowledge_base",
        "peekOfCode": "indian_history_kb = {\n    \" \": \"        \",\n    \"\": \"      \",\n    \"\": \"     \",\n    \"\": \"     \",\n    \"\": \"    \",\n    \"\": \"   \",\n    \"\": \"   \",\n    \"\": \"   24 \",\n    \"\": \"   ,        \",",
        "detail": "create_knowledge_base",
        "documentation": {}
    },
    {
        "label": "politics_kb",
        "kind": 5,
        "importPath": "create_knowledge_base",
        "description": "create_knowledge_base",
        "peekOfCode": "politics_kb = {\n    \"\": \",      \",\n    \"\": \"   ,       \",\n    \"\": \"   \",\n    \"\": \"   ,      \",\n    \"\": \"   \",\n    \" \": \"    \",\n    \"\": \"   \",\n    \" \": \"   \",\n    \"\": \"         \",",
        "detail": "create_knowledge_base",
        "documentation": {}
    },
    {
        "label": "world_gk_kb",
        "kind": 5,
        "importPath": "create_knowledge_base",
        "description": "create_knowledge_base",
        "peekOfCode": "world_gk_kb = {\n    \"\": \"       \",\n    \"\": \"      \",\n    \"\": \"   \",\n    \"\": \"    \",\n    \"\": \"     \",\n    \"\": \"    \",\n    \" \": \"    \",\n    \" \": \"    \",\n    \" \": \"    \",",
        "detail": "create_knowledge_base",
        "documentation": {}
    },
    {
        "label": "india_gk_kb",
        "kind": 5,
        "importPath": "create_knowledge_base",
        "description": "create_knowledge_base",
        "peekOfCode": "india_gk_kb = {\n    \"\": \"    ,        \",\n    \"  \": \" \",\n    \"\": \"  ,      \",\n    \"\": \" \",\n    \"\": \" \",\n    \"\": \"   \",\n    \" \": \"   \",\n    \"  \": \"  28   8    \",\n    \"\": \"         \",",
        "detail": "create_knowledge_base",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "intent_model",
        "description": "intent_model",
        "peekOfCode": "data = json.load(open(\"intent.json\", encoding=\"utf-8\"))\nX, y = [], []\nfor intent in data[\"intents\"]:\n    for p in intent[\"patterns\"]:\n        X.append(p)\n        y.append(intent[\"tag\"])\nvectorizer = TfidfVectorizer()\nXv = vectorizer.fit_transform(X)\nmodel = LogisticRegression()\nmodel.fit(Xv, y)",
        "detail": "intent_model",
        "documentation": {}
    },
    {
        "label": "vectorizer",
        "kind": 5,
        "importPath": "intent_model",
        "description": "intent_model",
        "peekOfCode": "vectorizer = TfidfVectorizer()\nXv = vectorizer.fit_transform(X)\nmodel = LogisticRegression()\nmodel.fit(Xv, y)\npickle.dump(model, open(\"intent_model.pkl\", \"wb\"))\npickle.dump(vectorizer, open(\"vectorizer.pkl\", \"wb\"))\nprint(\" Intent model trained\")",
        "detail": "intent_model",
        "documentation": {}
    },
    {
        "label": "Xv",
        "kind": 5,
        "importPath": "intent_model",
        "description": "intent_model",
        "peekOfCode": "Xv = vectorizer.fit_transform(X)\nmodel = LogisticRegression()\nmodel.fit(Xv, y)\npickle.dump(model, open(\"intent_model.pkl\", \"wb\"))\npickle.dump(vectorizer, open(\"vectorizer.pkl\", \"wb\"))\nprint(\" Intent model trained\")",
        "detail": "intent_model",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "intent_model",
        "description": "intent_model",
        "peekOfCode": "model = LogisticRegression()\nmodel.fit(Xv, y)\npickle.dump(model, open(\"intent_model.pkl\", \"wb\"))\npickle.dump(vectorizer, open(\"vectorizer.pkl\", \"wb\"))\nprint(\" Intent model trained\")",
        "detail": "intent_model",
        "documentation": {}
    },
    {
        "label": "get_intent",
        "kind": 2,
        "importPath": "intent_predict",
        "description": "intent_predict",
        "peekOfCode": "def get_intent(text):\n    if not MODEL_LOADED:\n        return None, 0.0\n    try:\n        # Vectorize input\n        Xv = vectorizer.transform([text])\n        # Predict\n        probabilities = model.predict_proba(Xv)[0]\n        max_idx = np.argmax(probabilities)\n        confidence = probabilities[max_idx]",
        "detail": "intent_predict",
        "documentation": {}
    },
    {
        "label": "KnowledgeBase",
        "kind": 6,
        "importPath": "knowledge_base",
        "description": "knowledge_base",
        "peekOfCode": "class KnowledgeBase:\n    def __init__(self):\n        \"\"\"Load all knowledge base files\"\"\"\n        self.history = self._load_kb(\"history_kb.pkl\")\n        self.indian_history = self._load_kb(\"indian_history_kb.pkl\")\n        self.politics = self._load_kb(\"politics_kb.pkl\")\n        self.world_gk = self._load_kb(\"world_gk_kb.pkl\")\n        self.india_gk = self._load_kb(\"india_gk_kb.pkl\")\n    def _load_kb(self, filename):\n        \"\"\"Load pickle file\"\"\"",
        "detail": "knowledge_base",
        "documentation": {}
    },
    {
        "label": "predict_intent",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def predict_intent(text):\n    X = vectorizer.transform([text])\n    intent = intent_model.predict(X)[0]\n    confidence = max(intent_model.predict_proba(X)[0])\n    return intent, confidence\n# =========================================\n# Load RAG\n# =========================================\n#rag = SimpleRAG(\"rag.jsonl\")\n# =========================================",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "qwen_reply",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def qwen_reply(hindi_text):\n    prompt = f\"\"\"     \n   \nUser: {hindi_text}\nAssistant:\"\"\"\n    result = llm(\n        prompt=prompt,\n        max_tokens=64,\n        temperature=0.6,\n        top_p=0.9,",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "intent_model",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "intent_model = pickle.load(open(\"intent_model.pkl\", \"rb\"))\nvectorizer = pickle.load(open(\"vectorizer.pkl\", \"rb\"))\n# [NEW] Load Deterministic Intents\nintent_map = nlu.load_intents(\"intent.json\")\nprint(f\"Loaded {len(intent_map)} deterministic keywords\")\n# [NEW] Load Knowledge Base\nkb = KnowledgeBase()\nprint(\" Knowledge Base loaded\")\ndef predict_intent(text):\n    X = vectorizer.transform([text])",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "vectorizer",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "vectorizer = pickle.load(open(\"vectorizer.pkl\", \"rb\"))\n# [NEW] Load Deterministic Intents\nintent_map = nlu.load_intents(\"intent.json\")\nprint(f\"Loaded {len(intent_map)} deterministic keywords\")\n# [NEW] Load Knowledge Base\nkb = KnowledgeBase()\nprint(\" Knowledge Base loaded\")\ndef predict_intent(text):\n    X = vectorizer.transform([text])\n    intent = intent_model.predict(X)[0]",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "intent_map",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "intent_map = nlu.load_intents(\"intent.json\")\nprint(f\"Loaded {len(intent_map)} deterministic keywords\")\n# [NEW] Load Knowledge Base\nkb = KnowledgeBase()\nprint(\" Knowledge Base loaded\")\ndef predict_intent(text):\n    X = vectorizer.transform([text])\n    intent = intent_model.predict(X)[0]\n    confidence = max(intent_model.predict_proba(X)[0])\n    return intent, confidence",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "kb",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "kb = KnowledgeBase()\nprint(\" Knowledge Base loaded\")\ndef predict_intent(text):\n    X = vectorizer.transform([text])\n    intent = intent_model.predict(X)[0]\n    confidence = max(intent_model.predict_proba(X)[0])\n    return intent, confidence\n# =========================================\n# Load RAG\n# =========================================",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "#rag",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "#rag = SimpleRAG(\"rag.jsonl\")\n# =========================================\n# Load Qwen ONCE\n# =========================================\nMODEL_PATH = \"Llama-3.2-1B-Instruct-Q4_K_M.gguf\"\nprint(\"Loading llama model...\")\nllm = Llama(\n    model_path=MODEL_PATH,\n    n_ctx=128,\n    n_threads=4,",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "MODEL_PATH",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "MODEL_PATH = \"Llama-3.2-1B-Instruct-Q4_K_M.gguf\"\nprint(\"Loading llama model...\")\nllm = Llama(\n    model_path=MODEL_PATH,\n    n_ctx=128,\n    n_threads=4,\n    n_batch=64,\n    verbose=False\n)\nprint(\"Qwen ready \")",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "llm = Llama(\n    model_path=MODEL_PATH,\n    n_ctx=128,\n    n_threads=4,\n    n_batch=64,\n    verbose=False\n)\nprint(\"Qwen ready \")\n# =========================================\n# Qwen fallback",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "load_intents",
        "kind": 2,
        "importPath": "nlu",
        "description": "nlu",
        "peekOfCode": "def load_intents(json_path):\n    \"\"\"\n    Loads intents from JSON and creates a mapping of word -> intent_tag.\n    \"\"\"\n    try:\n        with open(json_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        intent_map = {}\n        for intent in data['intents']:\n            tag = intent['tag']",
        "detail": "nlu",
        "documentation": {}
    },
    {
        "label": "preprocess",
        "kind": 2,
        "importPath": "nlu",
        "description": "nlu",
        "peekOfCode": "def preprocess(text):\n    \"\"\"\n    Lowercase, remove punctuation, strip whitespace.\n    \"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    text = text.lower()\n    # Remove punctuation using translation table\n    # Include common English and Hindi punctuation keys if needed, but standard string.punctuation handles standard ASCII\n    text = text.translate(str.maketrans('', '', string.punctuation))",
        "detail": "nlu",
        "documentation": {}
    },
    {
        "label": "tokenize",
        "kind": 2,
        "importPath": "nlu",
        "description": "nlu",
        "peekOfCode": "def tokenize(text):\n    \"\"\"\n    Split by whitespace.\n    \"\"\"\n    return text.split()\ndef filter_noise(tokens):\n    \"\"\"\n    Remove stopwords and short words.\n    \"\"\"\n    STOPWORDS = {",
        "detail": "nlu",
        "documentation": {}
    },
    {
        "label": "filter_noise",
        "kind": 2,
        "importPath": "nlu",
        "description": "nlu",
        "peekOfCode": "def filter_noise(tokens):\n    \"\"\"\n    Remove stopwords and short words.\n    \"\"\"\n    STOPWORDS = {\n        # Romanized\n        \"hai\", \"h\", \"ko\", \"se\", \"ka\", \"ki\", \"ke\", \"me\", \"mein\", \n        \"aur\", \"tathaa\", \"evam\", \"kyon\", \"kya\", \"kab\", \"kaise\", \n        \"kahan\", \"jab\", \"tab\", \"ab\", \"abhi\", \"bhi\", \"toh\", \"hi\", \n        \"ji\", \"sir\", \"madam\", \"sunie\", \"suno\", \"hey\", \"hello\", \"hi\", ",
        "detail": "nlu",
        "documentation": {}
    },
    {
        "label": "get_levenshtein_distance",
        "kind": 2,
        "importPath": "nlu",
        "description": "nlu",
        "peekOfCode": "def get_levenshtein_distance(s1, s2):\n    if len(s1) < len(s2):\n        return get_levenshtein_distance(s2, s1)\n    if len(s2) == 0:\n        return len(s1)\n    previous_row = range(len(s2) + 1)\n    for i, c1 in enumerate(s1):\n        current_row = [i + 1]\n        for j, c2 in enumerate(s2):\n            insertions = previous_row[j + 1] + 1",
        "detail": "nlu",
        "documentation": {}
    },
    {
        "label": "detect_intent",
        "kind": 2,
        "importPath": "nlu",
        "description": "nlu",
        "peekOfCode": "def detect_intent(text, intent_map):\n    \"\"\"\n    1. Preprocess & Tokenize\n    2. Filter Noise\n    3. Match (Exact then Fuzzy)\n    \"\"\"\n    clean_text = preprocess(text)\n    tokens = tokenize(clean_text)\n    clean_tokens = filter_noise(tokens)\n    # 1. Exact Match",
        "detail": "nlu",
        "documentation": {}
    },
    {
        "label": "fallback_to_llm",
        "kind": 2,
        "importPath": "nlu",
        "description": "nlu",
        "peekOfCode": "def fallback_to_llm(text):\n    \"\"\"\n    Generate a concise response using the LLM.\n    \"\"\"\n    prompt = f\"\"\"\n         \n           \n             \n    {text}\n    \"\"\"",
        "detail": "nlu",
        "documentation": {}
    },
    {
        "label": "time_now",
        "kind": 2,
        "importPath": "system_info",
        "description": "system_info",
        "peekOfCode": "def time_now(): return datetime.datetime.now().strftime(\"%H:%M:%S\")\ndef date_today(): return datetime.date.today().strftime(\"%d %B %Y\")\ndef day_today(): return datetime.datetime.now().strftime(\"%A\")\ndef uptime(): return str(datetime.datetime.now() -\n                        datetime.datetime.fromtimestamp(psutil.boot_time())).split('.')[0]\ndef cpu(): return f\"{psutil.cpu_percent()} %\"\ndef ram(): return f\"{psutil.virtual_memory().percent} %\"\ndef disk(): return f\"{psutil.disk_usage('/').percent} %\"\ndef battery():\n    b = psutil.sensors_battery()",
        "detail": "system_info",
        "documentation": {}
    },
    {
        "label": "date_today",
        "kind": 2,
        "importPath": "system_info",
        "description": "system_info",
        "peekOfCode": "def date_today(): return datetime.date.today().strftime(\"%d %B %Y\")\ndef day_today(): return datetime.datetime.now().strftime(\"%A\")\ndef uptime(): return str(datetime.datetime.now() -\n                        datetime.datetime.fromtimestamp(psutil.boot_time())).split('.')[0]\ndef cpu(): return f\"{psutil.cpu_percent()} %\"\ndef ram(): return f\"{psutil.virtual_memory().percent} %\"\ndef disk(): return f\"{psutil.disk_usage('/').percent} %\"\ndef battery():\n    b = psutil.sensors_battery()\n    return f\"{b.percent} %\" if b else \" \"",
        "detail": "system_info",
        "documentation": {}
    },
    {
        "label": "day_today",
        "kind": 2,
        "importPath": "system_info",
        "description": "system_info",
        "peekOfCode": "def day_today(): return datetime.datetime.now().strftime(\"%A\")\ndef uptime(): return str(datetime.datetime.now() -\n                        datetime.datetime.fromtimestamp(psutil.boot_time())).split('.')[0]\ndef cpu(): return f\"{psutil.cpu_percent()} %\"\ndef ram(): return f\"{psutil.virtual_memory().percent} %\"\ndef disk(): return f\"{psutil.disk_usage('/').percent} %\"\ndef battery():\n    b = psutil.sensors_battery()\n    return f\"{b.percent} %\" if b else \" \"\ndef temp():",
        "detail": "system_info",
        "documentation": {}
    },
    {
        "label": "uptime",
        "kind": 2,
        "importPath": "system_info",
        "description": "system_info",
        "peekOfCode": "def uptime(): return str(datetime.datetime.now() -\n                        datetime.datetime.fromtimestamp(psutil.boot_time())).split('.')[0]\ndef cpu(): return f\"{psutil.cpu_percent()} %\"\ndef ram(): return f\"{psutil.virtual_memory().percent} %\"\ndef disk(): return f\"{psutil.disk_usage('/').percent} %\"\ndef battery():\n    b = psutil.sensors_battery()\n    return f\"{b.percent} %\" if b else \" \"\ndef temp():\n    try:",
        "detail": "system_info",
        "documentation": {}
    },
    {
        "label": "cpu",
        "kind": 2,
        "importPath": "system_info",
        "description": "system_info",
        "peekOfCode": "def cpu(): return f\"{psutil.cpu_percent()} %\"\ndef ram(): return f\"{psutil.virtual_memory().percent} %\"\ndef disk(): return f\"{psutil.disk_usage('/').percent} %\"\ndef battery():\n    b = psutil.sensors_battery()\n    return f\"{b.percent} %\" if b else \" \"\ndef temp():\n    try:\n        return f\"{list(psutil.sensors_temperatures().values())[0][0].current} C\"\n    except:",
        "detail": "system_info",
        "documentation": {}
    },
    {
        "label": "ram",
        "kind": 2,
        "importPath": "system_info",
        "description": "system_info",
        "peekOfCode": "def ram(): return f\"{psutil.virtual_memory().percent} %\"\ndef disk(): return f\"{psutil.disk_usage('/').percent} %\"\ndef battery():\n    b = psutil.sensors_battery()\n    return f\"{b.percent} %\" if b else \" \"\ndef temp():\n    try:\n        return f\"{list(psutil.sensors_temperatures().values())[0][0].current} C\"\n    except:\n        return \" \"",
        "detail": "system_info",
        "documentation": {}
    },
    {
        "label": "disk",
        "kind": 2,
        "importPath": "system_info",
        "description": "system_info",
        "peekOfCode": "def disk(): return f\"{psutil.disk_usage('/').percent} %\"\ndef battery():\n    b = psutil.sensors_battery()\n    return f\"{b.percent} %\" if b else \" \"\ndef temp():\n    try:\n        return f\"{list(psutil.sensors_temperatures().values())[0][0].current} C\"\n    except:\n        return \" \"\ndef network():",
        "detail": "system_info",
        "documentation": {}
    },
    {
        "label": "battery",
        "kind": 2,
        "importPath": "system_info",
        "description": "system_info",
        "peekOfCode": "def battery():\n    b = psutil.sensors_battery()\n    return f\"{b.percent} %\" if b else \" \"\ndef temp():\n    try:\n        return f\"{list(psutil.sensors_temperatures().values())[0][0].current} C\"\n    except:\n        return \" \"\ndef network():\n    try:",
        "detail": "system_info",
        "documentation": {}
    },
    {
        "label": "temp",
        "kind": 2,
        "importPath": "system_info",
        "description": "system_info",
        "peekOfCode": "def temp():\n    try:\n        return f\"{list(psutil.sensors_temperatures().values())[0][0].current} C\"\n    except:\n        return \" \"\ndef network():\n    try:\n        socket.create_connection((\"8.8.8.8\", 53), timeout=2)\n        return \"  \"\n    except:",
        "detail": "system_info",
        "documentation": {}
    },
    {
        "label": "network",
        "kind": 2,
        "importPath": "system_info",
        "description": "system_info",
        "peekOfCode": "def network():\n    try:\n        socket.create_connection((\"8.8.8.8\", 53), timeout=2)\n        return \"  \"\n    except:\n        return \"  \"\ndef ip(): return socket.gethostbyname(socket.gethostname())\ndef hostname(): return socket.gethostname()",
        "detail": "system_info",
        "documentation": {}
    },
    {
        "label": "ip",
        "kind": 2,
        "importPath": "system_info",
        "description": "system_info",
        "peekOfCode": "def ip(): return socket.gethostbyname(socket.gethostname())\ndef hostname(): return socket.gethostname()",
        "detail": "system_info",
        "documentation": {}
    },
    {
        "label": "hostname",
        "kind": 2,
        "importPath": "system_info",
        "description": "system_info",
        "peekOfCode": "def hostname(): return socket.gethostname()",
        "detail": "system_info",
        "documentation": {}
    },
    {
        "label": "intent_map",
        "kind": 5,
        "importPath": "test_nlu",
        "description": "test_nlu",
        "peekOfCode": "intent_map = nlu.load_intents(\"intent.json\")\nprint(f\"Loaded {len(intent_map)} keywords.\")\n# Test inputs (Romanized examples from user + Devanagari actuals)\ntest_inputs = [\n    (\"asdh din\", None),          # mismatch (Roman 'din' vs Devanagari '')\n    (\"asdh \", \"day\"),         # Correct Hindi input\n    (\" \", \"day\"),         # \"batao\" is stopword, \"din\" matches\n    (\"random noise\", None),\n    (\"kuchhsh din\", None),       # Roman mismatch\n    (\" \", \"day\"),         # Devanagari noise + match",
        "detail": "test_nlu",
        "documentation": {}
    },
    {
        "label": "test_inputs",
        "kind": 5,
        "importPath": "test_nlu",
        "description": "test_nlu",
        "peekOfCode": "test_inputs = [\n    (\"asdh din\", None),          # mismatch (Roman 'din' vs Devanagari '')\n    (\"asdh \", \"day\"),         # Correct Hindi input\n    (\" \", \"day\"),         # \"batao\" is stopword, \"din\" matches\n    (\"random noise\", None),\n    (\"kuchhsh din\", None),       # Roman mismatch\n    (\" \", \"day\"),         # Devanagari noise + match\n    (\"\", \"time\"),\n    (\" \", \"exit\"),         # \"karo\" stopword, \"band\" () matches\n]",
        "detail": "test_nlu",
        "documentation": {}
    },
    {
        "label": "speak",
        "kind": 2,
        "importPath": "tts_piper",
        "description": "tts_piper",
        "peekOfCode": "def speak(text):\n    if not text or not text.strip():\n        return\n    if voice is None:\n        print(\"ERROR: Piper voice model not loaded\")\n        return\n    try:\n        print(\"Speaking text...\")\n    except UnicodeEncodeError:\n        print(\"Speaking text (Unicode)\")",
        "detail": "tts_piper",
        "documentation": {}
    },
    {
        "label": "PIPER_MODEL",
        "kind": 5,
        "importPath": "tts_piper",
        "description": "tts_piper",
        "peekOfCode": "PIPER_MODEL = \"piper/hi_IN-priyamvada-medium.onnx\"\n# Load the voice model once at module level for better performance\ntry:\n    voice = PiperVoice.load(PIPER_MODEL)\n    print(\"Piper voice model loaded successfully\")\nexcept Exception as e:\n    print(f\"Error loading Piper voice model: {e}\")\n    voice = None\ndef speak(text):\n    if not text or not text.strip():",
        "detail": "tts_piper",
        "documentation": {}
    },
    {
        "label": "test_phrases",
        "kind": 5,
        "importPath": "verify_intent",
        "description": "verify_intent",
        "peekOfCode": "test_phrases = [\n    \"aiva\",\n    \"hey aiva\",\n    \"hello aiva\",\n    \"what is time\",\n    \"open camera\",\n    \"random noise\",\n    \"aiva listen\"\n]\nprint(\"Testing Intent Model Prediction:\\n\")",
        "detail": "verify_intent",
        "documentation": {}
    },
    {
        "label": "detect_wake",
        "kind": 2,
        "importPath": "wake_fast",
        "description": "wake_fast",
        "peekOfCode": "def detect_wake(recognizer, data):\n    # Accept returns True if a full silence/end of utterance is reached\n    # but we are interested in partials for speed\n    if recognizer.AcceptWaveform(data):\n        result = json.loads(recognizer.Result())\n        text = result.get(\"text\", \"\")\n    else:\n        # PartialResult gives us real-time hypothesis\n        partial = recognizer.PartialResult()\n        text = json.loads(partial).get(\"partial\", \"\")",
        "detail": "wake_fast",
        "documentation": {}
    },
    {
        "label": "listen_for_wake",
        "kind": 2,
        "importPath": "wake_fast",
        "description": "wake_fast",
        "peekOfCode": "def listen_for_wake():\n    if not rec:\n        return False\n    p = pyaudio.PyAudio()\n    stream = p.open(\n        format=pyaudio.paInt16,\n        channels=1,\n        rate=SAMPLE_RATE,\n        input=True,\n        frames_per_buffer=BLOCK_SIZE",
        "detail": "wake_fast",
        "documentation": {}
    },
    {
        "label": "WAKE_WORDS",
        "kind": 5,
        "importPath": "wake_fast",
        "description": "wake_fast",
        "peekOfCode": "WAKE_WORDS = [\"\", \"iva\", \"kira\", \"ava\", \"hyva\", \"hey nova\", \"hey\"]  # check multiple\nMODEL_PATH = \"vosk-model-small-hi-0.22\"\nSAMPLE_RATE = 16000\nBLOCK_SIZE = 1024   # fast\n# =========================================\n# Load Vosk model\n# =========================================\ntry:\n    model = Model(MODEL_PATH)\n    rec = KaldiRecognizer(model, SAMPLE_RATE)",
        "detail": "wake_fast",
        "documentation": {}
    },
    {
        "label": "MODEL_PATH",
        "kind": 5,
        "importPath": "wake_fast",
        "description": "wake_fast",
        "peekOfCode": "MODEL_PATH = \"vosk-model-small-hi-0.22\"\nSAMPLE_RATE = 16000\nBLOCK_SIZE = 1024   # fast\n# =========================================\n# Load Vosk model\n# =========================================\ntry:\n    model = Model(MODEL_PATH)\n    rec = KaldiRecognizer(model, SAMPLE_RATE)\n    print(\" Vosk (Fast) model loaded\")",
        "detail": "wake_fast",
        "documentation": {}
    },
    {
        "label": "SAMPLE_RATE",
        "kind": 5,
        "importPath": "wake_fast",
        "description": "wake_fast",
        "peekOfCode": "SAMPLE_RATE = 16000\nBLOCK_SIZE = 1024   # fast\n# =========================================\n# Load Vosk model\n# =========================================\ntry:\n    model = Model(MODEL_PATH)\n    rec = KaldiRecognizer(model, SAMPLE_RATE)\n    print(\" Vosk (Fast) model loaded\")\nexcept Exception as e:",
        "detail": "wake_fast",
        "documentation": {}
    },
    {
        "label": "BLOCK_SIZE",
        "kind": 5,
        "importPath": "wake_fast",
        "description": "wake_fast",
        "peekOfCode": "BLOCK_SIZE = 1024   # fast\n# =========================================\n# Load Vosk model\n# =========================================\ntry:\n    model = Model(MODEL_PATH)\n    rec = KaldiRecognizer(model, SAMPLE_RATE)\n    print(\" Vosk (Fast) model loaded\")\nexcept Exception as e:\n    print(\" Failed to load Vosk model:\", e)",
        "detail": "wake_fast",
        "documentation": {}
    },
    {
        "label": "tokenize",
        "kind": 2,
        "importPath": "wake_vosk",
        "description": "wake_vosk",
        "peekOfCode": "def tokenize(text):\n    return text.strip().split()\n# =========================================\n# Audio device helper\n# =========================================\ndef get_input_device_index():\n    # IMPORTANT:\n    # If mic does not work, replace None with the correct mic index\n    return None\n# =========================================",
        "detail": "wake_vosk",
        "documentation": {}
    },
    {
        "label": "get_input_device_index",
        "kind": 2,
        "importPath": "wake_vosk",
        "description": "wake_vosk",
        "peekOfCode": "def get_input_device_index():\n    # IMPORTANT:\n    # If mic does not work, replace None with the correct mic index\n    return None\n# =========================================\n# Listen loop\n# =========================================\nimport time  # [NEW]\ndef listen_loop(timeout=None):  # [NEW] timeout support\n    if not rec:",
        "detail": "wake_vosk",
        "documentation": {}
    },
    {
        "label": "listen_loop",
        "kind": 2,
        "importPath": "wake_vosk",
        "description": "wake_vosk",
        "peekOfCode": "def listen_loop(timeout=None):  # [NEW] timeout support\n    if not rec:\n        print(\" Vosk not ready\")\n        return None\n    rec.Reset()\n    try:\n        p = pyaudio.PyAudio()\n        stream = p.open(\n            format=pyaudio.paInt16,\n            channels=1,",
        "detail": "wake_vosk",
        "documentation": {}
    },
    {
        "label": "WAKE_WORDS",
        "kind": 5,
        "importPath": "wake_vosk",
        "description": "wake_vosk",
        "peekOfCode": "WAKE_WORDS = [\"\", \"iva\", \"kira\", \"ava\", \"hyva\", \"hey nova\", \"hey\"]  # check multiple\nMODEL_PATH = \"vosk-model-small-hi-0.22\"\nSAMPLE_RATE = 16000\nBLOCK_SIZE = 4000          # 0.5 sec\nRMS_THRESHOLD = 30         # VERY LOW (important)\nMIN_TOKENS = 1             # allow even 1-word speech\n# =========================================\n# Load Vosk model ONCE\n# =========================================\ntry:",
        "detail": "wake_vosk",
        "documentation": {}
    },
    {
        "label": "MODEL_PATH",
        "kind": 5,
        "importPath": "wake_vosk",
        "description": "wake_vosk",
        "peekOfCode": "MODEL_PATH = \"vosk-model-small-hi-0.22\"\nSAMPLE_RATE = 16000\nBLOCK_SIZE = 4000          # 0.5 sec\nRMS_THRESHOLD = 30         # VERY LOW (important)\nMIN_TOKENS = 1             # allow even 1-word speech\n# =========================================\n# Load Vosk model ONCE\n# =========================================\ntry:\n    model = Model(MODEL_PATH)",
        "detail": "wake_vosk",
        "documentation": {}
    },
    {
        "label": "SAMPLE_RATE",
        "kind": 5,
        "importPath": "wake_vosk",
        "description": "wake_vosk",
        "peekOfCode": "SAMPLE_RATE = 16000\nBLOCK_SIZE = 4000          # 0.5 sec\nRMS_THRESHOLD = 30         # VERY LOW (important)\nMIN_TOKENS = 1             # allow even 1-word speech\n# =========================================\n# Load Vosk model ONCE\n# =========================================\ntry:\n    model = Model(MODEL_PATH)\n    rec = KaldiRecognizer(model, SAMPLE_RATE)",
        "detail": "wake_vosk",
        "documentation": {}
    },
    {
        "label": "BLOCK_SIZE",
        "kind": 5,
        "importPath": "wake_vosk",
        "description": "wake_vosk",
        "peekOfCode": "BLOCK_SIZE = 4000          # 0.5 sec\nRMS_THRESHOLD = 30         # VERY LOW (important)\nMIN_TOKENS = 1             # allow even 1-word speech\n# =========================================\n# Load Vosk model ONCE\n# =========================================\ntry:\n    model = Model(MODEL_PATH)\n    rec = KaldiRecognizer(model, SAMPLE_RATE)\n    rec.SetWords(False)",
        "detail": "wake_vosk",
        "documentation": {}
    },
    {
        "label": "RMS_THRESHOLD",
        "kind": 5,
        "importPath": "wake_vosk",
        "description": "wake_vosk",
        "peekOfCode": "RMS_THRESHOLD = 30         # VERY LOW (important)\nMIN_TOKENS = 1             # allow even 1-word speech\n# =========================================\n# Load Vosk model ONCE\n# =========================================\ntry:\n    model = Model(MODEL_PATH)\n    rec = KaldiRecognizer(model, SAMPLE_RATE)\n    rec.SetWords(False)\n    rec.SetPartialWords(False)",
        "detail": "wake_vosk",
        "documentation": {}
    },
    {
        "label": "MIN_TOKENS",
        "kind": 5,
        "importPath": "wake_vosk",
        "description": "wake_vosk",
        "peekOfCode": "MIN_TOKENS = 1             # allow even 1-word speech\n# =========================================\n# Load Vosk model ONCE\n# =========================================\ntry:\n    model = Model(MODEL_PATH)\n    rec = KaldiRecognizer(model, SAMPLE_RATE)\n    rec.SetWords(False)\n    rec.SetPartialWords(False)\n    print(\" Vosk model loaded\")",
        "detail": "wake_vosk",
        "documentation": {}
    }
]